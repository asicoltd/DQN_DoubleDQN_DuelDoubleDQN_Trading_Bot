{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d103862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import os\n",
    "# Configuration\n",
    "TICKER = 'BTCUSDT'\n",
    "SEQ_LENGTH = 60\n",
    "PREDICTION_STEPS = 10\n",
    "INITIAL_BALANCE = 10000.0\n",
    "TRADE_FEE_RATE = 0.0001\n",
    "MIN_TRADE_AMOUNT = 10\n",
    "GRID_COUNT = 20\n",
    "EPOCHS = 0\n",
    "EPISODES = 100\n",
    "BATCH_SIZE = 256\n",
    "MEMORY_SIZE = 10000\n",
    "GAMMA = 0.95\n",
    "EPSILON_DECAY = 0.999995\n",
    "MIN_EPSILON = 0.001\n",
    "LEARNING_RATE = 0.0001\n",
    "TARGET_UPDATE_FREQ = 100\n",
    "LSTM_LEARNING_RATE = 10\n",
    "LSTM_PATH = 'lstm_predictor_fixed_grid2.pth'\n",
    "DDDQN_PATH = 'dueldoubledqn_agent_fixed_grid.pth'\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, filepath):\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        \n",
    "    def preprocess(self):\n",
    "        df = self.data.copy()\n",
    "        # Feature engineering\n",
    "        df['Returns'] = df['close'].pct_change()\n",
    "        df['Volatility'] = df['Returns'].rolling(20).std()\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # Scale features\n",
    "        scaled = self.scaler.fit_transform(df[['open', 'high', 'low', 'close', 'volume', 'Returns', 'Volatility']])\n",
    "        return scaled, df['close'].values\n",
    "    \n",
    "    def create_sequences(self, data, target):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - SEQ_LENGTH - PREDICTION_STEPS):\n",
    "            X.append(data[i:i+SEQ_LENGTH])\n",
    "            y.append(target[i+SEQ_LENGTH+PREDICTION_STEPS-1])\n",
    "        return np.array(X), np.array(y)\n",
    "class AdaptiveLRScheduler:\n",
    "    def __init__(self, optimizer, patience=5, factor=0.5, min_lr=1e-20, max_lr=1.0, threshold=1e-4, fluctuation_threshold=0.2):\n",
    "        self.optimizer = optimizer\n",
    "        self.patience = patience\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.threshold = threshold\n",
    "        self.fluctuation_threshold = fluctuation_threshold\n",
    "        \n",
    "        self.loss_history = []\n",
    "        self.wait = 0\n",
    "        self.best_loss = float('inf')\n",
    "    def step(self, current_loss):\n",
    "        self.loss_history.append(current_loss)\n",
    "\n",
    "        if len(self.loss_history) > 2:\n",
    "            fluctuation = abs(self.loss_history[-1] - self.loss_history[-2]) / (self.loss_history[-2] + 1e-8)\n",
    "        else:\n",
    "            fluctuation = 0\n",
    "\n",
    "        # 1. Detect fluctuations\n",
    "        if fluctuation > self.fluctuation_threshold:\n",
    "            self._reduce_lr(\"High fluctuation detected\")\n",
    "            self.wait = 0\n",
    "            return\n",
    "\n",
    "        # 2. Detect improvement\n",
    "        improvement = self.best_loss - current_loss\n",
    "        if improvement > self.threshold:\n",
    "            self.best_loss = current_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self._reduce_lr(\"Loss plateau detected\")\n",
    "                self.wait = 0\n",
    "\n",
    "    def _reduce_lr(self, reason):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            old_lr = param_group['lr']\n",
    "            new_lr = max(old_lr * self.factor, self.min_lr)\n",
    "            param_group['lr'] = new_lr\n",
    "        print(f\"[LR SCHEDULER] {reason}. LR reduced to: {new_lr:.10f}\")\n",
    "        \n",
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=50, num_layers=2):\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def save(self, path, best_loss):\n",
    "        torch.save({\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'best_loss': best_loss\n",
    "        }, path)\n",
    "        print(best_loss)\n",
    "    def load(self, path, map_location=None):\n",
    "        if os.path.exists(path):\n",
    "            checkpoint = torch.load(path, map_location=map_location)\n",
    "            self.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.eval()  # Optional: switch to eval mode.\n",
    "            return checkpoint.get('best_loss', float('inf'))\n",
    "        else:\n",
    "            print(f\"⚠️ Warning: No checkpoint found at {path}.\")\n",
    "            return float('inf')\n",
    "\n",
    "        \n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(state_size, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "        q_values = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "        return q_values\n",
    "\n",
    "\n",
    "class TradingEnvironment:\n",
    "    def __init__(self, data, prices, predictor, grids):\n",
    "        self.data = data\n",
    "        self.prices = prices\n",
    "        self.predictor = predictor\n",
    "        self.grids = grids\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = SEQ_LENGTH\n",
    "        self.balance = INITIAL_BALANCE\n",
    "        self.btc_held = 0.0\n",
    "        self.portfolio_value = [INITIAL_BALANCE]\n",
    "        self.trades = []\n",
    "        self.done = False\n",
    "        self.prev_portfolio_value = INITIAL_BALANCE   # <-- Add this line\n",
    "        return self.get_state()\n",
    "\n",
    "    \n",
    "    def get_state(self):\n",
    "        current_data = self.data[self.current_step]\n",
    "        price = self.prices[self.current_step]\n",
    "        grid_position = np.digitize(price, self.grids) / GRID_COUNT\n",
    "        \n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            seq_data = self.data[self.current_step-SEQ_LENGTH:self.current_step]\n",
    "            seq_tensor = torch.tensor(seq_data[np.newaxis, ...], dtype=torch.float32).to(device)\n",
    "            prediction = self.predictor(seq_tensor).cpu().item()\n",
    "        \n",
    "        return np.array([\n",
    "            *current_data,\n",
    "            self.balance / INITIAL_BALANCE,\n",
    "            self.btc_held * price / INITIAL_BALANCE,\n",
    "            grid_position,\n",
    "            prediction / price\n",
    "        ])\n",
    "    \n",
    "    def execute_trade(self, action, amount):\n",
    "        fee = amount * TRADE_FEE_RATE\n",
    "        return amount - fee\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Actions:\n",
    "        0 = hold\n",
    "        1 = buy\n",
    "        2 = sell\n",
    "        \"\"\"\n",
    "        price = self.prices[self.current_step]\n",
    "        grid_position = np.digitize(price, self.grids) / len(self.grids)  # normalized 0-1\n",
    "        \n",
    "        trade_amount = 0\n",
    "        min_trade_usd = 10  # Minimum USD amount per trade, for example\n",
    "        min_trade_btc = 0.001  # Minimum BTC amount per trade\n",
    "        \n",
    "        if action == 1:  # Buy\n",
    "            # Buy more when price is near lower grids (grid_position close to 0)\n",
    "            multiplier = max(0.1, 1 - grid_position)  # from 1 at bottom grid to 0.1 near top grid\n",
    "            amount_to_spend = self.balance * multiplier\n",
    "            trade_amount = max(amount_to_spend, min_trade_usd)\n",
    "            \n",
    "            btc_to_buy = trade_amount / price\n",
    "            btc_to_buy = min(btc_to_buy, self.balance / price)  # Can't buy more than balance allows\n",
    "            btc_to_buy = max(btc_to_buy, min_trade_btc)\n",
    "            \n",
    "            # Execute buy\n",
    "            cost = btc_to_buy * price\n",
    "            if cost <= self.balance:\n",
    "                self.balance -= cost\n",
    "                self.btc_held += btc_to_buy\n",
    "                self.trades.append(('buy', price, self.current_step))\n",
    "        \n",
    "        elif action == 2:  # Sell\n",
    "            # Sell more when price is near upper grids (grid_position close to 1)\n",
    "            multiplier = max(0.1, grid_position)  # from 0.1 at bottom grid to 1 near top grid\n",
    "            btc_to_sell = self.btc_held * multiplier\n",
    "            btc_to_sell = max(btc_to_sell, min_trade_btc)\n",
    "            \n",
    "            # Execute sell\n",
    "            if btc_to_sell <= self.btc_held:\n",
    "                revenue = btc_to_sell * price\n",
    "                self.balance += revenue\n",
    "                self.btc_held -= btc_to_sell\n",
    "                self.trades.append(('sell', price, self.current_step))\n",
    "        \n",
    "        else:\n",
    "            # Hold - no trade\n",
    "            pass\n",
    "        \n",
    "        # Update portfolio value, rewards, etc.\n",
    "        self.current_step += 1\n",
    "        self.done = self.current_step >= len(self.prices) - 1\n",
    "        \n",
    "        portfolio_value = self.balance + self.btc_held * price\n",
    "        reward = portfolio_value - self.prev_portfolio_value\n",
    "        self.prev_portfolio_value = portfolio_value\n",
    "        \n",
    "        next_state = self.get_state()\n",
    "        \n",
    "        return next_state, reward, self.done, portfolio_value\n",
    "\n",
    "class AdaptiveTradingEnvironment(TradingEnvironment):\n",
    "    def __init__(self, data, prices, predictor, initial_grids, \n",
    "                 recalibration_interval=100, lookback_window=500):\n",
    "        self.original_grids = initial_grids\n",
    "        self.recalibration_interval = recalibration_interval\n",
    "        self.lookback_window = lookback_window\n",
    "        self.steps_since_recalibration = 0\n",
    "        self.grid_history = [initial_grids.copy()]\n",
    "        super().__init__(data, prices, predictor, initial_grids)\n",
    "    def reset(self):\n",
    "        self.steps_since_recalibration = 0\n",
    "        self.grids = self.original_grids.copy()\n",
    "        return super().reset()\n",
    "    \n",
    "    def recalculate_grids(self):\n",
    "        \"\"\"Recalculate grids based on recent price window\"\"\"\n",
    "        start_idx = max(0, self.current_step - self.lookback_window)\n",
    "        recent_prices = self.prices[start_idx:self.current_step]\n",
    "        \n",
    "        # Use percentiles to avoid extreme values\n",
    "        low = np.percentile(recent_prices, 10)  # 10th percentile\n",
    "        high = np.percentile(recent_prices, 90)  # 90th percentile\n",
    "        \n",
    "        # Safety checks\n",
    "        if high <= low:\n",
    "            high = low * 1.2  # Prevent invalid range\n",
    "            \n",
    "        new_grids = np.linspace(low, high, num=GRID_COUNT+1)[1:-1]\n",
    "        self.grid_history.append(new_grids)\n",
    "        return new_grids\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Recalculate grids periodically\n",
    "        self.steps_since_recalibration += 1\n",
    "        if self.steps_since_recalibration >= self.recalibration_interval:\n",
    "            self.grids = self.recalculate_grids()\n",
    "            self.steps_since_recalibration = 0\n",
    "        \n",
    "        return super().step(action)\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.epsilon = 1\n",
    "        self.model = DuelingDQN(state_size, action_size).to(device)\n",
    "        self.target_model = DuelingDQN(state_size, action_size).to(device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.target_model.eval()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        self.scheduler = StepLR(self.optimizer, step_size=5, gamma=0.95)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.steps = 0\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.model(state_tensor)\n",
    "            return torch.argmax(q_values).item()\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1).to(device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Q(s,a)\n",
    "        current_q = self.model(states).gather(1, actions)\n",
    "\n",
    "        # Double DQN target\n",
    "        with torch.no_grad():\n",
    "            # Select best action using online network\n",
    "            best_actions = self.model(next_states).argmax(1, keepdim=True)\n",
    "            # Evaluate using target network\n",
    "            next_q = self.target_model(next_states).gather(1, best_actions)\n",
    "            target_q = rewards.unsqueeze(1) + (1 - dones.unsqueeze(1)) * GAMMA * next_q\n",
    "\n",
    "        loss = self.criterion(current_q, target_q)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "\n",
    "        self.steps += 1\n",
    "        if self.steps % TARGET_UPDATE_FREQ == 0:\n",
    "            self.update_target_model()\n",
    "\n",
    "        if self.epsilon > MIN_EPSILON:\n",
    "            self.epsilon *= EPSILON_DECAY\n",
    "\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        \n",
    "    \n",
    "    def load(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "\n",
    "def create_grids(prices, num_grids=GRID_COUNT):\n",
    "    low = np.percentile(prices, 5)\n",
    "    high = np.percentile(prices, 95)\n",
    "    return np.linspace(low, high, num=num_grids+1)[1:-1]\n",
    "\n",
    "def plot_grid_history(env):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(env.prices, label='Price', alpha=0.5)\n",
    "    \n",
    "    # Plot all grid levels over time\n",
    "    for i, grids in enumerate(env.grid_history):\n",
    "        if i % 5 == 0:  # Plot every 5th recalibration for clarity\n",
    "            step = i * env.recalibration_interval\n",
    "            for grid in grids:\n",
    "                plt.axhline(y=grid, color='gray', alpha=0.1)\n",
    "    \n",
    "    plt.title('Adaptive Grid Levels Over Time')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def train_lstm(X_train, y_train, input_size, epochs=20, batch_size=64):\n",
    "    model = LSTMPredictor(input_size).to(device)\n",
    "    best_val_loss = model.load(LSTM_PATH, map_location=device)\n",
    "    print('loaded', best_val_loss)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LSTM_LEARNING_RATE)\n",
    "    scheduler = AdaptiveLRScheduler(optimizer, patience=10, factor=0.5)\n",
    "    # Convert to tensors\n",
    "    X_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    \n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss/len(loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "        scheduler.step(avg_loss)\n",
    "        if avg_loss < best_val_loss:\n",
    "            best_val_loss = avg_loss\n",
    "            model.save(LSTM_PATH, best_val_loss)\n",
    "            print('saved')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0981f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from binance.client import Client\n",
    "import pandas as pd\n",
    "import os\n",
    "from requests.exceptions import ReadTimeout, RequestException\n",
    "\n",
    "client = Client()\n",
    "\n",
    "\n",
    "def get_binance_data(symbol=TICKER, interval=Client.KLINE_INTERVAL_1MINUTE, lookback='60', retries=3, delay=20):\n",
    "    \"\"\"Fetch latest Binance OHLCV data with retry mechanism.\"\"\"\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            klines = client.get_klines(symbol=symbol, interval=interval, limit=int(lookback))\n",
    "            break  # Break if successful\n",
    "        except (ReadTimeout, RequestException) as e:\n",
    "            attempt += 1\n",
    "            print(f\"[Attempt {attempt}/{retries}] Error fetching data: {e}. Retrying in {delay} seconds...\")\n",
    "            time.sleep(delay)\n",
    "    else:\n",
    "        print(\"Failed to fetch Binance data after multiple retries.\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame on failure\n",
    "\n",
    "    df = pd.DataFrame(klines, columns=[\n",
    "        'timestamp', 'open', 'high', 'low', 'close', 'volume',\n",
    "        'close_time', 'quote_asset_volume', 'num_trades',\n",
    "        'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'\n",
    "    ])\n",
    "\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']].astype({\n",
    "        'open': float, 'high': float, 'low': float, 'close': float, 'volume': float\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30b3824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Iteration 1 ===\n",
      "Training LSTM Price Predictor...\n",
      "loaded 95962.83333333333\n",
      "\n",
      "Training DQN Agent...\n",
      "Episode 1/100, Reward: 69.5964, Portfolio: $10,069.60, Epsilon: 0.4983\n",
      "Episode 2/100, Reward: 96.5700, Portfolio: $10,096.57, Epsilon: 0.4961\n",
      "Episode 3/100, Reward: 176.8797, Portfolio: $10,176.88, Epsilon: 0.4938\n",
      "Episode 4/100, Reward: 171.0051, Portfolio: $10,171.01, Epsilon: 0.4915\n",
      "Episode 5/100, Reward: 80.3829, Portfolio: $10,080.38, Epsilon: 0.4893\n",
      "Episode 6/100, Reward: 136.5704, Portfolio: $10,136.57, Epsilon: 0.4870\n",
      "Episode 7/100, Reward: 49.4308, Portfolio: $10,049.43, Epsilon: 0.4848\n",
      "Episode 8/100, Reward: 182.3378, Portfolio: $10,182.34, Epsilon: 0.4826\n",
      "Episode 9/100, Reward: 131.9867, Portfolio: $10,131.99, Epsilon: 0.4804\n",
      "Episode 10/100, Reward: 107.9413, Portfolio: $10,107.94, Epsilon: 0.4782\n",
      "Models saved successfully\n",
      "Episode 11/100, Reward: 187.7825, Portfolio: $10,187.78, Epsilon: 0.4760\n",
      "Episode 12/100, Reward: 81.6436, Portfolio: $10,081.64, Epsilon: 0.4738\n",
      "Episode 13/100, Reward: 81.8467, Portfolio: $10,081.85, Epsilon: 0.4716\n",
      "Episode 14/100, Reward: 103.8475, Portfolio: $10,103.85, Epsilon: 0.4694\n",
      "Episode 15/100, Reward: 109.3760, Portfolio: $10,109.38, Epsilon: 0.4673\n",
      "Episode 16/100, Reward: 57.7149, Portfolio: $10,057.71, Epsilon: 0.4652\n",
      "Episode 17/100, Reward: 123.7702, Portfolio: $10,123.77, Epsilon: 0.4630\n",
      "Episode 18/100, Reward: 66.8512, Portfolio: $10,066.85, Epsilon: 0.4609\n",
      "Episode 19/100, Reward: 123.4639, Portfolio: $10,123.46, Epsilon: 0.4588\n",
      "Episode 20/100, Reward: 105.1794, Portfolio: $10,105.18, Epsilon: 0.4567\n",
      "Models saved successfully\n",
      "Episode 21/100, Reward: 42.0835, Portfolio: $10,042.08, Epsilon: 0.4546\n",
      "Episode 22/100, Reward: 34.2137, Portfolio: $10,034.21, Epsilon: 0.4525\n",
      "Episode 23/100, Reward: 79.9832, Portfolio: $10,079.98, Epsilon: 0.4504\n",
      "Episode 24/100, Reward: 121.9462, Portfolio: $10,121.95, Epsilon: 0.4484\n",
      "Episode 25/100, Reward: 111.1468, Portfolio: $10,111.15, Epsilon: 0.4463\n",
      "Episode 26/100, Reward: 64.4619, Portfolio: $10,064.46, Epsilon: 0.4443\n",
      "Episode 27/100, Reward: 110.5302, Portfolio: $10,110.53, Epsilon: 0.4422\n",
      "Episode 28/100, Reward: 56.4223, Portfolio: $10,056.42, Epsilon: 0.4402\n",
      "Episode 29/100, Reward: 56.3180, Portfolio: $10,056.32, Epsilon: 0.4382\n",
      "Episode 30/100, Reward: 163.9464, Portfolio: $10,163.95, Epsilon: 0.4362\n",
      "Models saved successfully\n",
      "Episode 31/100, Reward: 103.4968, Portfolio: $10,103.50, Epsilon: 0.4342\n",
      "Episode 32/100, Reward: 29.1050, Portfolio: $10,029.10, Epsilon: 0.4322\n",
      "Episode 33/100, Reward: 132.6213, Portfolio: $10,132.62, Epsilon: 0.4302\n",
      "Episode 34/100, Reward: 79.6023, Portfolio: $10,079.60, Epsilon: 0.4282\n",
      "Episode 35/100, Reward: 71.7935, Portfolio: $10,071.79, Epsilon: 0.4263\n",
      "Episode 36/100, Reward: -17.0851, Portfolio: $9,982.91, Epsilon: 0.4243\n",
      "Episode 37/100, Reward: 71.5555, Portfolio: $10,071.56, Epsilon: 0.4224\n",
      "Episode 38/100, Reward: 66.7474, Portfolio: $10,066.75, Epsilon: 0.4204\n",
      "Episode 39/100, Reward: 61.3985, Portfolio: $10,061.40, Epsilon: 0.4185\n",
      "Episode 40/100, Reward: 16.4759, Portfolio: $10,016.48, Epsilon: 0.4166\n",
      "Models saved successfully\n",
      "Episode 41/100, Reward: 92.9478, Portfolio: $10,092.95, Epsilon: 0.4147\n",
      "Episode 42/100, Reward: 104.2571, Portfolio: $10,104.26, Epsilon: 0.4128\n",
      "Episode 43/100, Reward: 110.5989, Portfolio: $10,110.60, Epsilon: 0.4109\n",
      "Episode 44/100, Reward: 74.5065, Portfolio: $10,074.51, Epsilon: 0.4090\n",
      "Episode 45/100, Reward: 90.4985, Portfolio: $10,090.50, Epsilon: 0.4071\n",
      "Episode 46/100, Reward: 51.9810, Portfolio: $10,051.98, Epsilon: 0.4053\n",
      "Episode 47/100, Reward: 77.9658, Portfolio: $10,077.97, Epsilon: 0.4034\n",
      "Episode 48/100, Reward: 88.8520, Portfolio: $10,088.85, Epsilon: 0.4015\n",
      "Episode 49/100, Reward: 157.1972, Portfolio: $10,157.20, Epsilon: 0.3997\n",
      "Episode 50/100, Reward: 88.9828, Portfolio: $10,088.98, Epsilon: 0.3979\n",
      "Models saved successfully\n",
      "Episode 51/100, Reward: 105.2954, Portfolio: $10,105.30, Epsilon: 0.3961\n",
      "Episode 52/100, Reward: 96.0696, Portfolio: $10,096.07, Epsilon: 0.3942\n",
      "Episode 53/100, Reward: 136.5038, Portfolio: $10,136.50, Epsilon: 0.3924\n",
      "Episode 54/100, Reward: 64.5139, Portfolio: $10,064.51, Epsilon: 0.3906\n",
      "Episode 55/100, Reward: 45.7864, Portfolio: $10,045.79, Epsilon: 0.3888\n",
      "Episode 56/100, Reward: -11.8659, Portfolio: $9,988.13, Epsilon: 0.3871\n",
      "Episode 57/100, Reward: -7.1625, Portfolio: $9,992.84, Epsilon: 0.3853\n",
      "Episode 58/100, Reward: 86.7055, Portfolio: $10,086.71, Epsilon: 0.3835\n",
      "Episode 59/100, Reward: 107.6575, Portfolio: $10,107.66, Epsilon: 0.3818\n",
      "Episode 60/100, Reward: 47.5164, Portfolio: $10,047.52, Epsilon: 0.3800\n",
      "Models saved successfully\n",
      "Episode 61/100, Reward: 134.0350, Portfolio: $10,134.03, Epsilon: 0.3783\n",
      "Episode 62/100, Reward: 124.2077, Portfolio: $10,124.21, Epsilon: 0.3765\n",
      "Episode 63/100, Reward: 62.7632, Portfolio: $10,062.76, Epsilon: 0.3748\n",
      "Episode 64/100, Reward: 159.5755, Portfolio: $10,159.58, Epsilon: 0.3731\n",
      "Episode 65/100, Reward: 52.0861, Portfolio: $10,052.09, Epsilon: 0.3714\n",
      "Episode 66/100, Reward: 46.3188, Portfolio: $10,046.32, Epsilon: 0.3697\n",
      "Episode 67/100, Reward: 65.3713, Portfolio: $10,065.37, Epsilon: 0.3680\n",
      "Episode 68/100, Reward: 69.7799, Portfolio: $10,069.78, Epsilon: 0.3663\n",
      "Episode 69/100, Reward: 34.4058, Portfolio: $10,034.41, Epsilon: 0.3646\n",
      "Episode 70/100, Reward: 38.2533, Portfolio: $10,038.25, Epsilon: 0.3629\n",
      "Models saved successfully\n",
      "Episode 71/100, Reward: 132.2497, Portfolio: $10,132.25, Epsilon: 0.3613\n",
      "Episode 72/100, Reward: 121.0614, Portfolio: $10,121.06, Epsilon: 0.3596\n",
      "Episode 73/100, Reward: 139.3425, Portfolio: $10,139.34, Epsilon: 0.3580\n",
      "Episode 74/100, Reward: 80.3833, Portfolio: $10,080.38, Epsilon: 0.3563\n",
      "Episode 75/100, Reward: 59.4693, Portfolio: $10,059.47, Epsilon: 0.3547\n",
      "Episode 76/100, Reward: 128.0610, Portfolio: $10,128.06, Epsilon: 0.3531\n",
      "Episode 77/100, Reward: 71.2162, Portfolio: $10,071.22, Epsilon: 0.3515\n",
      "Episode 78/100, Reward: 38.0727, Portfolio: $10,038.07, Epsilon: 0.3498\n",
      "Episode 79/100, Reward: 52.4996, Portfolio: $10,052.50, Epsilon: 0.3482\n",
      "Episode 80/100, Reward: 53.8194, Portfolio: $10,053.82, Epsilon: 0.3466\n",
      "Models saved successfully\n",
      "Episode 81/100, Reward: 85.0957, Portfolio: $10,085.10, Epsilon: 0.3451\n",
      "Episode 82/100, Reward: 78.4165, Portfolio: $10,078.42, Epsilon: 0.3435\n",
      "Episode 83/100, Reward: 80.3304, Portfolio: $10,080.33, Epsilon: 0.3419\n",
      "Episode 84/100, Reward: 133.4447, Portfolio: $10,133.44, Epsilon: 0.3403\n",
      "Episode 85/100, Reward: 48.5110, Portfolio: $10,048.51, Epsilon: 0.3388\n",
      "Episode 86/100, Reward: 87.4251, Portfolio: $10,087.43, Epsilon: 0.3372\n",
      "Episode 87/100, Reward: 69.5199, Portfolio: $10,069.52, Epsilon: 0.3357\n",
      "Episode 88/100, Reward: 42.9875, Portfolio: $10,042.99, Epsilon: 0.3341\n",
      "Episode 89/100, Reward: 77.9041, Portfolio: $10,077.90, Epsilon: 0.3326\n",
      "Episode 90/100, Reward: 63.1891, Portfolio: $10,063.19, Epsilon: 0.3311\n",
      "Models saved successfully\n",
      "Episode 91/100, Reward: 75.0086, Portfolio: $10,075.01, Epsilon: 0.3296\n",
      "Episode 92/100, Reward: 31.2588, Portfolio: $10,031.26, Epsilon: 0.3280\n",
      "Episode 93/100, Reward: 42.3228, Portfolio: $10,042.32, Epsilon: 0.3265\n",
      "Episode 94/100, Reward: 35.4293, Portfolio: $10,035.43, Epsilon: 0.3250\n",
      "Episode 95/100, Reward: 108.9963, Portfolio: $10,109.00, Epsilon: 0.3236\n",
      "Episode 96/100, Reward: 89.0181, Portfolio: $10,089.02, Epsilon: 0.3221\n",
      "Episode 97/100, Reward: 112.0322, Portfolio: $10,112.03, Epsilon: 0.3206\n",
      "Episode 98/100, Reward: 77.4433, Portfolio: $10,077.44, Epsilon: 0.3191\n",
      "Episode 99/100, Reward: 44.8555, Portfolio: $10,044.86, Epsilon: 0.3177\n",
      "Episode 100/100, Reward: 72.1820, Portfolio: $10,072.18, Epsilon: 0.3162\n",
      "Models saved successfully\n",
      "\n",
      "Evaluating on test data...\n",
      "\n",
      "==================================================\n",
      "Initial Balance: $10,000.00\n",
      "Final Portfolio Value: $9,997.34\n",
      "Total Return: -0.03%\n",
      "Total Trades Executed: 114\n",
      "Grid Levels: [103505.25 103610.59 103715.94 103821.28 103926.63 104031.98 104137.32\n",
      " 104242.67 104348.02 104453.36 104558.71 104664.05 104769.4  104874.75\n",
      " 104980.09 105085.44 105190.78 105296.13 105401.48]\n",
      "=== Iteration 2 ===\n",
      "Training LSTM Price Predictor...\n",
      "loaded 95962.83333333333\n",
      "\n",
      "Training DQN Agent...\n",
      "Episode 1/100, Reward: 113.2312, Portfolio: $10,113.23, Epsilon: 0.4983\n",
      "Episode 2/100, Reward: 221.9870, Portfolio: $10,221.99, Epsilon: 0.4961\n",
      "Episode 3/100, Reward: 225.7156, Portfolio: $10,225.72, Epsilon: 0.4938\n",
      "Episode 4/100, Reward: 145.8078, Portfolio: $10,145.81, Epsilon: 0.4915\n",
      "Episode 5/100, Reward: 140.0296, Portfolio: $10,140.03, Epsilon: 0.4893\n",
      "Episode 6/100, Reward: 218.0148, Portfolio: $10,218.01, Epsilon: 0.4870\n",
      "Episode 7/100, Reward: 210.4008, Portfolio: $10,210.40, Epsilon: 0.4848\n",
      "Episode 8/100, Reward: 200.7297, Portfolio: $10,200.73, Epsilon: 0.4826\n",
      "Episode 9/100, Reward: 193.6911, Portfolio: $10,193.69, Epsilon: 0.4804\n",
      "Episode 10/100, Reward: 193.0182, Portfolio: $10,193.02, Epsilon: 0.4782\n",
      "Models saved successfully\n",
      "Episode 11/100, Reward: 157.1811, Portfolio: $10,157.18, Epsilon: 0.4760\n",
      "Episode 12/100, Reward: 251.3709, Portfolio: $10,251.37, Epsilon: 0.4738\n",
      "Episode 13/100, Reward: 214.6442, Portfolio: $10,214.64, Epsilon: 0.4716\n",
      "Episode 14/100, Reward: 156.0706, Portfolio: $10,156.07, Epsilon: 0.4694\n",
      "Episode 15/100, Reward: 172.0470, Portfolio: $10,172.05, Epsilon: 0.4673\n",
      "Episode 16/100, Reward: 155.3086, Portfolio: $10,155.31, Epsilon: 0.4652\n",
      "Episode 17/100, Reward: 221.5496, Portfolio: $10,221.55, Epsilon: 0.4630\n",
      "Episode 18/100, Reward: 344.6401, Portfolio: $10,344.64, Epsilon: 0.4609\n",
      "Episode 19/100, Reward: 200.6238, Portfolio: $10,200.62, Epsilon: 0.4588\n",
      "Episode 20/100, Reward: 130.7170, Portfolio: $10,130.72, Epsilon: 0.4567\n",
      "Models saved successfully\n",
      "Episode 21/100, Reward: 167.2090, Portfolio: $10,167.21, Epsilon: 0.4546\n",
      "Episode 22/100, Reward: 226.9292, Portfolio: $10,226.93, Epsilon: 0.4525\n",
      "Episode 23/100, Reward: 178.4498, Portfolio: $10,178.45, Epsilon: 0.4504\n",
      "Episode 24/100, Reward: 185.4527, Portfolio: $10,185.45, Epsilon: 0.4484\n",
      "Episode 25/100, Reward: 186.0447, Portfolio: $10,186.04, Epsilon: 0.4463\n",
      "Episode 26/100, Reward: 166.4224, Portfolio: $10,166.42, Epsilon: 0.4443\n",
      "Episode 27/100, Reward: 237.0621, Portfolio: $10,237.06, Epsilon: 0.4422\n",
      "Episode 28/100, Reward: 215.0819, Portfolio: $10,215.08, Epsilon: 0.4402\n",
      "Episode 29/100, Reward: 137.8445, Portfolio: $10,137.84, Epsilon: 0.4382\n",
      "Episode 30/100, Reward: 159.0888, Portfolio: $10,159.09, Epsilon: 0.4362\n",
      "Models saved successfully\n",
      "Episode 31/100, Reward: 175.4938, Portfolio: $10,175.49, Epsilon: 0.4342\n",
      "Episode 32/100, Reward: 162.6940, Portfolio: $10,162.69, Epsilon: 0.4322\n",
      "Episode 33/100, Reward: 149.6698, Portfolio: $10,149.67, Epsilon: 0.4302\n",
      "Episode 34/100, Reward: 139.8982, Portfolio: $10,139.90, Epsilon: 0.4282\n",
      "Episode 35/100, Reward: 181.4343, Portfolio: $10,181.43, Epsilon: 0.4263\n",
      "Episode 36/100, Reward: 277.6228, Portfolio: $10,277.62, Epsilon: 0.4243\n",
      "Episode 37/100, Reward: 234.3364, Portfolio: $10,234.34, Epsilon: 0.4224\n",
      "Episode 38/100, Reward: 193.5616, Portfolio: $10,193.56, Epsilon: 0.4204\n",
      "Episode 39/100, Reward: 138.3837, Portfolio: $10,138.38, Epsilon: 0.4185\n",
      "Episode 40/100, Reward: 218.0268, Portfolio: $10,218.03, Epsilon: 0.4166\n",
      "Models saved successfully\n",
      "Episode 41/100, Reward: 112.5891, Portfolio: $10,112.59, Epsilon: 0.4147\n",
      "Episode 42/100, Reward: 144.0121, Portfolio: $10,144.01, Epsilon: 0.4128\n",
      "Episode 43/100, Reward: 228.4661, Portfolio: $10,228.47, Epsilon: 0.4109\n",
      "Episode 44/100, Reward: 172.8240, Portfolio: $10,172.82, Epsilon: 0.4090\n",
      "Episode 45/100, Reward: 278.1694, Portfolio: $10,278.17, Epsilon: 0.4071\n",
      "Episode 46/100, Reward: 86.6871, Portfolio: $10,086.69, Epsilon: 0.4053\n",
      "Episode 47/100, Reward: 103.0712, Portfolio: $10,103.07, Epsilon: 0.4034\n",
      "Episode 48/100, Reward: 188.3764, Portfolio: $10,188.38, Epsilon: 0.4015\n",
      "Episode 49/100, Reward: 125.3199, Portfolio: $10,125.32, Epsilon: 0.3997\n",
      "Episode 50/100, Reward: 141.3559, Portfolio: $10,141.36, Epsilon: 0.3979\n",
      "Models saved successfully\n",
      "Episode 51/100, Reward: 178.1616, Portfolio: $10,178.16, Epsilon: 0.3961\n",
      "Episode 52/100, Reward: 188.8286, Portfolio: $10,188.83, Epsilon: 0.3942\n",
      "Episode 53/100, Reward: 171.8142, Portfolio: $10,171.81, Epsilon: 0.3924\n",
      "Episode 54/100, Reward: 196.4983, Portfolio: $10,196.50, Epsilon: 0.3906\n",
      "Episode 55/100, Reward: 144.7444, Portfolio: $10,144.74, Epsilon: 0.3888\n",
      "Episode 56/100, Reward: 230.3711, Portfolio: $10,230.37, Epsilon: 0.3871\n",
      "Episode 57/100, Reward: 220.1159, Portfolio: $10,220.12, Epsilon: 0.3853\n",
      "Episode 58/100, Reward: 246.4953, Portfolio: $10,246.50, Epsilon: 0.3835\n",
      "Episode 59/100, Reward: 120.2729, Portfolio: $10,120.27, Epsilon: 0.3818\n",
      "Episode 60/100, Reward: 177.0668, Portfolio: $10,177.07, Epsilon: 0.3800\n",
      "Models saved successfully\n",
      "Episode 61/100, Reward: 130.7902, Portfolio: $10,130.79, Epsilon: 0.3783\n",
      "Episode 62/100, Reward: 185.7963, Portfolio: $10,185.80, Epsilon: 0.3765\n",
      "Episode 63/100, Reward: 217.7694, Portfolio: $10,217.77, Epsilon: 0.3748\n",
      "Episode 64/100, Reward: 141.9433, Portfolio: $10,141.94, Epsilon: 0.3731\n",
      "Episode 65/100, Reward: 223.6134, Portfolio: $10,223.61, Epsilon: 0.3714\n",
      "Episode 66/100, Reward: 238.6537, Portfolio: $10,238.65, Epsilon: 0.3697\n",
      "Episode 67/100, Reward: 206.8034, Portfolio: $10,206.80, Epsilon: 0.3680\n",
      "Episode 68/100, Reward: 138.7711, Portfolio: $10,138.77, Epsilon: 0.3663\n",
      "Episode 69/100, Reward: 126.4771, Portfolio: $10,126.48, Epsilon: 0.3646\n",
      "Episode 70/100, Reward: 167.5855, Portfolio: $10,167.59, Epsilon: 0.3629\n",
      "Models saved successfully\n",
      "Episode 71/100, Reward: 244.4410, Portfolio: $10,244.44, Epsilon: 0.3613\n",
      "Episode 72/100, Reward: 120.3715, Portfolio: $10,120.37, Epsilon: 0.3596\n",
      "Episode 73/100, Reward: 163.5345, Portfolio: $10,163.53, Epsilon: 0.3580\n",
      "Episode 74/100, Reward: 122.1204, Portfolio: $10,122.12, Epsilon: 0.3563\n",
      "Episode 75/100, Reward: 156.3201, Portfolio: $10,156.32, Epsilon: 0.3547\n",
      "Episode 76/100, Reward: 175.6840, Portfolio: $10,175.68, Epsilon: 0.3531\n",
      "Episode 77/100, Reward: 181.1887, Portfolio: $10,181.19, Epsilon: 0.3515\n",
      "Episode 78/100, Reward: 201.3904, Portfolio: $10,201.39, Epsilon: 0.3498\n",
      "Episode 79/100, Reward: 184.5129, Portfolio: $10,184.51, Epsilon: 0.3482\n",
      "Episode 80/100, Reward: 217.2624, Portfolio: $10,217.26, Epsilon: 0.3466\n",
      "Models saved successfully\n",
      "Episode 81/100, Reward: 264.6571, Portfolio: $10,264.66, Epsilon: 0.3451\n",
      "Episode 82/100, Reward: 229.7599, Portfolio: $10,229.76, Epsilon: 0.3435\n",
      "Episode 83/100, Reward: 195.0864, Portfolio: $10,195.09, Epsilon: 0.3419\n",
      "Episode 84/100, Reward: 201.6035, Portfolio: $10,201.60, Epsilon: 0.3403\n",
      "Episode 85/100, Reward: 150.7769, Portfolio: $10,150.78, Epsilon: 0.3388\n",
      "Episode 86/100, Reward: 132.0763, Portfolio: $10,132.08, Epsilon: 0.3372\n",
      "Episode 87/100, Reward: 135.5715, Portfolio: $10,135.57, Epsilon: 0.3357\n",
      "Episode 88/100, Reward: 199.3055, Portfolio: $10,199.31, Epsilon: 0.3341\n",
      "Episode 89/100, Reward: 155.5479, Portfolio: $10,155.55, Epsilon: 0.3326\n",
      "Episode 90/100, Reward: 247.1455, Portfolio: $10,247.15, Epsilon: 0.3311\n",
      "Models saved successfully\n",
      "Episode 91/100, Reward: 157.3215, Portfolio: $10,157.32, Epsilon: 0.3296\n",
      "Episode 92/100, Reward: 207.5089, Portfolio: $10,207.51, Epsilon: 0.3280\n",
      "Episode 93/100, Reward: 188.2815, Portfolio: $10,188.28, Epsilon: 0.3265\n",
      "Episode 94/100, Reward: 239.3710, Portfolio: $10,239.37, Epsilon: 0.3250\n",
      "Episode 95/100, Reward: 256.9208, Portfolio: $10,256.92, Epsilon: 0.3236\n",
      "Episode 96/100, Reward: 150.3486, Portfolio: $10,150.35, Epsilon: 0.3221\n",
      "Episode 97/100, Reward: 184.8291, Portfolio: $10,184.83, Epsilon: 0.3206\n",
      "Episode 98/100, Reward: 100.2847, Portfolio: $10,100.28, Epsilon: 0.3191\n",
      "Episode 99/100, Reward: 174.3477, Portfolio: $10,174.35, Epsilon: 0.3177\n",
      "Episode 100/100, Reward: 83.6443, Portfolio: $10,083.64, Epsilon: 0.3162\n",
      "Models saved successfully\n",
      "\n",
      "Evaluating on test data...\n",
      "\n",
      "==================================================\n",
      "Initial Balance: $10,000.00\n",
      "Final Portfolio Value: $10,047.36\n",
      "Total Return: 0.47%\n",
      "Total Trades Executed: 27\n",
      "Grid Levels: [103510.25 103620.61 103730.96 103841.31 103951.67 104062.02 104172.38\n",
      " 104282.73 104393.08 104503.44 104613.79 104724.14 104834.5  104944.85\n",
      " 105055.2  105165.56 105275.91 105386.26 105496.62]\n",
      "=== Iteration 3 ===\n",
      "Training LSTM Price Predictor...\n",
      "loaded 95962.83333333333\n",
      "\n",
      "Training DQN Agent...\n",
      "Episode 1/100, Reward: 168.7221, Portfolio: $10,168.72, Epsilon: 0.4983\n",
      "Episode 2/100, Reward: 206.1291, Portfolio: $10,206.13, Epsilon: 0.4961\n",
      "Episode 3/100, Reward: 185.3798, Portfolio: $10,185.38, Epsilon: 0.4938\n",
      "Episode 4/100, Reward: 160.2911, Portfolio: $10,160.29, Epsilon: 0.4915\n",
      "Episode 5/100, Reward: 131.2683, Portfolio: $10,131.27, Epsilon: 0.4893\n",
      "Episode 6/100, Reward: 142.8841, Portfolio: $10,142.88, Epsilon: 0.4870\n",
      "Episode 7/100, Reward: 160.9073, Portfolio: $10,160.91, Epsilon: 0.4848\n",
      "Episode 8/100, Reward: 188.4431, Portfolio: $10,188.44, Epsilon: 0.4826\n",
      "Episode 9/100, Reward: 112.1115, Portfolio: $10,112.11, Epsilon: 0.4804\n",
      "Episode 10/100, Reward: 134.7123, Portfolio: $10,134.71, Epsilon: 0.4782\n",
      "Models saved successfully\n",
      "Episode 11/100, Reward: 145.9564, Portfolio: $10,145.96, Epsilon: 0.4760\n",
      "Episode 12/100, Reward: 134.0885, Portfolio: $10,134.09, Epsilon: 0.4738\n",
      "Episode 13/100, Reward: 186.5701, Portfolio: $10,186.57, Epsilon: 0.4716\n",
      "Episode 14/100, Reward: 139.1858, Portfolio: $10,139.19, Epsilon: 0.4694\n",
      "Episode 15/100, Reward: 159.8968, Portfolio: $10,159.90, Epsilon: 0.4673\n",
      "Episode 16/100, Reward: 168.6533, Portfolio: $10,168.65, Epsilon: 0.4652\n",
      "Episode 17/100, Reward: 150.8604, Portfolio: $10,150.86, Epsilon: 0.4630\n",
      "Episode 18/100, Reward: 180.5790, Portfolio: $10,180.58, Epsilon: 0.4609\n",
      "Episode 19/100, Reward: 141.9198, Portfolio: $10,141.92, Epsilon: 0.4588\n",
      "Episode 20/100, Reward: 120.0820, Portfolio: $10,120.08, Epsilon: 0.4567\n",
      "Models saved successfully\n",
      "Episode 21/100, Reward: 181.3924, Portfolio: $10,181.39, Epsilon: 0.4546\n",
      "Episode 22/100, Reward: 149.9670, Portfolio: $10,149.97, Epsilon: 0.4525\n",
      "Episode 23/100, Reward: 124.1357, Portfolio: $10,124.14, Epsilon: 0.4504\n",
      "Episode 24/100, Reward: 199.2801, Portfolio: $10,199.28, Epsilon: 0.4484\n",
      "Episode 25/100, Reward: 232.3271, Portfolio: $10,232.33, Epsilon: 0.4463\n",
      "Episode 26/100, Reward: 231.0012, Portfolio: $10,231.00, Epsilon: 0.4443\n",
      "Episode 27/100, Reward: 180.1800, Portfolio: $10,180.18, Epsilon: 0.4422\n",
      "Episode 28/100, Reward: 106.7264, Portfolio: $10,106.73, Epsilon: 0.4402\n",
      "Episode 29/100, Reward: 164.8445, Portfolio: $10,164.84, Epsilon: 0.4382\n",
      "Episode 30/100, Reward: 158.7736, Portfolio: $10,158.77, Epsilon: 0.4362\n",
      "Models saved successfully\n",
      "Episode 31/100, Reward: 173.4258, Portfolio: $10,173.43, Epsilon: 0.4342\n",
      "Episode 32/100, Reward: 117.7207, Portfolio: $10,117.72, Epsilon: 0.4322\n",
      "Episode 33/100, Reward: 225.7514, Portfolio: $10,225.75, Epsilon: 0.4302\n",
      "Episode 34/100, Reward: 213.4544, Portfolio: $10,213.45, Epsilon: 0.4282\n",
      "Episode 35/100, Reward: 128.0738, Portfolio: $10,128.07, Epsilon: 0.4263\n",
      "Episode 36/100, Reward: 206.5748, Portfolio: $10,206.57, Epsilon: 0.4243\n",
      "Episode 37/100, Reward: 234.8848, Portfolio: $10,234.88, Epsilon: 0.4224\n",
      "Episode 38/100, Reward: 221.1813, Portfolio: $10,221.18, Epsilon: 0.4204\n",
      "Episode 39/100, Reward: 202.6576, Portfolio: $10,202.66, Epsilon: 0.4185\n",
      "Episode 40/100, Reward: 176.8520, Portfolio: $10,176.85, Epsilon: 0.4166\n",
      "Models saved successfully\n",
      "Episode 41/100, Reward: 182.0683, Portfolio: $10,182.07, Epsilon: 0.4147\n",
      "Episode 42/100, Reward: 159.6771, Portfolio: $10,159.68, Epsilon: 0.4128\n",
      "Episode 43/100, Reward: 164.1287, Portfolio: $10,164.13, Epsilon: 0.4109\n",
      "Episode 44/100, Reward: 188.7044, Portfolio: $10,188.70, Epsilon: 0.4090\n",
      "Episode 45/100, Reward: 147.5056, Portfolio: $10,147.51, Epsilon: 0.4071\n",
      "Episode 46/100, Reward: 217.4430, Portfolio: $10,217.44, Epsilon: 0.4053\n",
      "Episode 47/100, Reward: 152.6715, Portfolio: $10,152.67, Epsilon: 0.4034\n",
      "Episode 48/100, Reward: 144.6927, Portfolio: $10,144.69, Epsilon: 0.4015\n",
      "Episode 49/100, Reward: 200.6619, Portfolio: $10,200.66, Epsilon: 0.3997\n",
      "Episode 50/100, Reward: 142.0373, Portfolio: $10,142.04, Epsilon: 0.3979\n",
      "Models saved successfully\n",
      "Episode 51/100, Reward: 188.9428, Portfolio: $10,188.94, Epsilon: 0.3961\n",
      "Episode 52/100, Reward: 174.2754, Portfolio: $10,174.28, Epsilon: 0.3942\n",
      "Episode 53/100, Reward: 123.1661, Portfolio: $10,123.17, Epsilon: 0.3924\n",
      "Episode 54/100, Reward: 78.2255, Portfolio: $10,078.23, Epsilon: 0.3906\n",
      "Episode 55/100, Reward: 184.4722, Portfolio: $10,184.47, Epsilon: 0.3888\n",
      "Episode 56/100, Reward: 183.9481, Portfolio: $10,183.95, Epsilon: 0.3871\n",
      "Episode 57/100, Reward: 161.1898, Portfolio: $10,161.19, Epsilon: 0.3853\n",
      "Episode 58/100, Reward: 196.2636, Portfolio: $10,196.26, Epsilon: 0.3835\n",
      "Episode 59/100, Reward: 129.4328, Portfolio: $10,129.43, Epsilon: 0.3818\n",
      "Episode 60/100, Reward: 165.5964, Portfolio: $10,165.60, Epsilon: 0.3800\n",
      "Models saved successfully\n",
      "Episode 61/100, Reward: 184.8349, Portfolio: $10,184.83, Epsilon: 0.3783\n",
      "Episode 62/100, Reward: 136.9581, Portfolio: $10,136.96, Epsilon: 0.3765\n",
      "Episode 63/100, Reward: 200.5614, Portfolio: $10,200.56, Epsilon: 0.3748\n",
      "Episode 64/100, Reward: 188.2329, Portfolio: $10,188.23, Epsilon: 0.3731\n",
      "Episode 65/100, Reward: 185.6428, Portfolio: $10,185.64, Epsilon: 0.3714\n",
      "Episode 66/100, Reward: 191.6253, Portfolio: $10,191.63, Epsilon: 0.3697\n",
      "Episode 67/100, Reward: 145.1430, Portfolio: $10,145.14, Epsilon: 0.3680\n",
      "Episode 68/100, Reward: 146.1294, Portfolio: $10,146.13, Epsilon: 0.3663\n",
      "Episode 69/100, Reward: 189.6460, Portfolio: $10,189.65, Epsilon: 0.3646\n",
      "Episode 70/100, Reward: 134.6847, Portfolio: $10,134.68, Epsilon: 0.3629\n",
      "Models saved successfully\n",
      "Episode 71/100, Reward: 136.6448, Portfolio: $10,136.64, Epsilon: 0.3613\n",
      "Episode 72/100, Reward: 174.0930, Portfolio: $10,174.09, Epsilon: 0.3596\n",
      "Episode 73/100, Reward: 152.0606, Portfolio: $10,152.06, Epsilon: 0.3580\n",
      "Episode 74/100, Reward: 175.9980, Portfolio: $10,176.00, Epsilon: 0.3563\n",
      "Episode 75/100, Reward: 105.2930, Portfolio: $10,105.29, Epsilon: 0.3547\n",
      "Episode 76/100, Reward: 173.6794, Portfolio: $10,173.68, Epsilon: 0.3531\n",
      "Episode 77/100, Reward: 158.8556, Portfolio: $10,158.86, Epsilon: 0.3515\n",
      "Episode 78/100, Reward: 209.6959, Portfolio: $10,209.70, Epsilon: 0.3498\n",
      "Episode 79/100, Reward: 160.4339, Portfolio: $10,160.43, Epsilon: 0.3482\n",
      "Episode 80/100, Reward: 167.7030, Portfolio: $10,167.70, Epsilon: 0.3466\n",
      "Models saved successfully\n",
      "Episode 81/100, Reward: 122.0518, Portfolio: $10,122.05, Epsilon: 0.3451\n",
      "Episode 82/100, Reward: 184.1745, Portfolio: $10,184.17, Epsilon: 0.3435\n",
      "Episode 83/100, Reward: 154.8101, Portfolio: $10,154.81, Epsilon: 0.3419\n",
      "Episode 84/100, Reward: 173.0071, Portfolio: $10,173.01, Epsilon: 0.3403\n",
      "Episode 85/100, Reward: 154.1687, Portfolio: $10,154.17, Epsilon: 0.3388\n",
      "Episode 86/100, Reward: 181.2729, Portfolio: $10,181.27, Epsilon: 0.3372\n",
      "Episode 87/100, Reward: 155.4524, Portfolio: $10,155.45, Epsilon: 0.3357\n",
      "Episode 88/100, Reward: 193.7797, Portfolio: $10,193.78, Epsilon: 0.3341\n",
      "Episode 89/100, Reward: 177.1347, Portfolio: $10,177.13, Epsilon: 0.3326\n",
      "Episode 90/100, Reward: 145.0896, Portfolio: $10,145.09, Epsilon: 0.3311\n",
      "Models saved successfully\n",
      "Episode 91/100, Reward: 238.0428, Portfolio: $10,238.04, Epsilon: 0.3296\n",
      "Episode 92/100, Reward: 132.2815, Portfolio: $10,132.28, Epsilon: 0.3280\n",
      "Episode 93/100, Reward: 164.5593, Portfolio: $10,164.56, Epsilon: 0.3265\n",
      "Episode 94/100, Reward: 151.9603, Portfolio: $10,151.96, Epsilon: 0.3250\n",
      "Episode 95/100, Reward: 116.5849, Portfolio: $10,116.58, Epsilon: 0.3236\n",
      "Episode 96/100, Reward: 151.4611, Portfolio: $10,151.46, Epsilon: 0.3221\n",
      "Episode 97/100, Reward: 229.9045, Portfolio: $10,229.90, Epsilon: 0.3206\n",
      "Episode 98/100, Reward: 165.0201, Portfolio: $10,165.02, Epsilon: 0.3191\n",
      "Episode 99/100, Reward: 165.2868, Portfolio: $10,165.29, Epsilon: 0.3177\n",
      "Episode 100/100, Reward: 191.0594, Portfolio: $10,191.06, Epsilon: 0.3162\n",
      "Models saved successfully\n",
      "\n",
      "Evaluating on test data...\n",
      "\n",
      "==================================================\n",
      "Initial Balance: $10,000.00\n",
      "Final Portfolio Value: $10,071.45\n",
      "Total Return: 0.71%\n",
      "Total Trades Executed: 67\n",
      "Grid Levels: [103698.77 103799.89 103901.01 104002.14 104103.26 104204.38 104305.5\n",
      " 104406.63 104507.75 104608.87 104709.99 104811.12 104912.24 105013.36\n",
      " 105114.48 105215.6  105316.73 105417.85 105518.97]\n",
      "=== Iteration 4 ===\n",
      "Training LSTM Price Predictor...\n",
      "loaded 95962.83333333333\n",
      "\n",
      "Training DQN Agent...\n",
      "Episode 1/100, Reward: 205.9795, Portfolio: $10,205.98, Epsilon: 0.4983\n",
      "Episode 2/100, Reward: 268.2257, Portfolio: $10,268.23, Epsilon: 0.4961\n",
      "Episode 3/100, Reward: 195.4678, Portfolio: $10,195.47, Epsilon: 0.4938\n",
      "Episode 4/100, Reward: 173.9999, Portfolio: $10,174.00, Epsilon: 0.4915\n",
      "Episode 5/100, Reward: 215.9983, Portfolio: $10,216.00, Epsilon: 0.4893\n",
      "Episode 6/100, Reward: 198.0392, Portfolio: $10,198.04, Epsilon: 0.4870\n",
      "Episode 7/100, Reward: 167.6286, Portfolio: $10,167.63, Epsilon: 0.4848\n",
      "Episode 8/100, Reward: 116.6443, Portfolio: $10,116.64, Epsilon: 0.4826\n",
      "Episode 9/100, Reward: 162.0746, Portfolio: $10,162.07, Epsilon: 0.4804\n",
      "Episode 10/100, Reward: 208.1999, Portfolio: $10,208.20, Epsilon: 0.4782\n",
      "Models saved successfully\n",
      "Episode 11/100, Reward: 202.8650, Portfolio: $10,202.87, Epsilon: 0.4760\n",
      "Episode 12/100, Reward: 184.8381, Portfolio: $10,184.84, Epsilon: 0.4738\n",
      "Episode 13/100, Reward: 201.1792, Portfolio: $10,201.18, Epsilon: 0.4716\n",
      "Episode 14/100, Reward: 169.7390, Portfolio: $10,169.74, Epsilon: 0.4694\n",
      "Episode 15/100, Reward: 125.9366, Portfolio: $10,125.94, Epsilon: 0.4673\n",
      "Episode 16/100, Reward: 163.5952, Portfolio: $10,163.60, Epsilon: 0.4652\n",
      "Episode 17/100, Reward: 87.8790, Portfolio: $10,087.88, Epsilon: 0.4630\n",
      "Episode 18/100, Reward: 148.2694, Portfolio: $10,148.27, Epsilon: 0.4609\n",
      "Episode 19/100, Reward: 191.4681, Portfolio: $10,191.47, Epsilon: 0.4588\n",
      "Episode 20/100, Reward: 142.7971, Portfolio: $10,142.80, Epsilon: 0.4567\n",
      "Models saved successfully\n",
      "Episode 21/100, Reward: 164.9357, Portfolio: $10,164.94, Epsilon: 0.4546\n",
      "Episode 22/100, Reward: 135.8702, Portfolio: $10,135.87, Epsilon: 0.4525\n",
      "Episode 23/100, Reward: 220.2197, Portfolio: $10,220.22, Epsilon: 0.4504\n",
      "Episode 24/100, Reward: 191.4749, Portfolio: $10,191.47, Epsilon: 0.4484\n",
      "Episode 25/100, Reward: 153.2358, Portfolio: $10,153.24, Epsilon: 0.4463\n",
      "Episode 26/100, Reward: 178.8801, Portfolio: $10,178.88, Epsilon: 0.4443\n",
      "Episode 27/100, Reward: 237.0545, Portfolio: $10,237.05, Epsilon: 0.4422\n",
      "Episode 28/100, Reward: 123.8699, Portfolio: $10,123.87, Epsilon: 0.4402\n",
      "Episode 29/100, Reward: 198.2298, Portfolio: $10,198.23, Epsilon: 0.4382\n",
      "Episode 30/100, Reward: 234.9937, Portfolio: $10,234.99, Epsilon: 0.4362\n",
      "Models saved successfully\n",
      "Episode 31/100, Reward: 154.0112, Portfolio: $10,154.01, Epsilon: 0.4342\n",
      "Episode 32/100, Reward: 209.5900, Portfolio: $10,209.59, Epsilon: 0.4322\n",
      "Episode 33/100, Reward: 162.7321, Portfolio: $10,162.73, Epsilon: 0.4302\n",
      "Episode 34/100, Reward: 155.2043, Portfolio: $10,155.20, Epsilon: 0.4282\n",
      "Episode 35/100, Reward: 101.4782, Portfolio: $10,101.48, Epsilon: 0.4263\n",
      "Episode 36/100, Reward: 191.2608, Portfolio: $10,191.26, Epsilon: 0.4243\n",
      "Episode 37/100, Reward: 181.9543, Portfolio: $10,181.95, Epsilon: 0.4224\n",
      "Episode 38/100, Reward: 158.8469, Portfolio: $10,158.85, Epsilon: 0.4204\n",
      "Episode 39/100, Reward: 142.3819, Portfolio: $10,142.38, Epsilon: 0.4185\n",
      "Episode 40/100, Reward: 134.9297, Portfolio: $10,134.93, Epsilon: 0.4166\n",
      "Models saved successfully\n",
      "Episode 41/100, Reward: 147.3924, Portfolio: $10,147.39, Epsilon: 0.4147\n",
      "Episode 42/100, Reward: 61.6037, Portfolio: $10,061.60, Epsilon: 0.4128\n",
      "Episode 43/100, Reward: 141.7924, Portfolio: $10,141.79, Epsilon: 0.4109\n",
      "Episode 44/100, Reward: 218.2591, Portfolio: $10,218.26, Epsilon: 0.4090\n",
      "Episode 45/100, Reward: 168.2352, Portfolio: $10,168.24, Epsilon: 0.4071\n",
      "Episode 46/100, Reward: 69.5353, Portfolio: $10,069.54, Epsilon: 0.4053\n",
      "Episode 47/100, Reward: 108.3634, Portfolio: $10,108.36, Epsilon: 0.4034\n",
      "Episode 48/100, Reward: 113.7391, Portfolio: $10,113.74, Epsilon: 0.4015\n",
      "Episode 49/100, Reward: 176.4088, Portfolio: $10,176.41, Epsilon: 0.3997\n",
      "Episode 50/100, Reward: 161.3336, Portfolio: $10,161.33, Epsilon: 0.3979\n",
      "Models saved successfully\n",
      "Episode 51/100, Reward: 147.1268, Portfolio: $10,147.13, Epsilon: 0.3961\n",
      "Episode 52/100, Reward: 222.1552, Portfolio: $10,222.16, Epsilon: 0.3942\n",
      "Episode 53/100, Reward: 196.9466, Portfolio: $10,196.95, Epsilon: 0.3924\n",
      "Episode 54/100, Reward: 92.6080, Portfolio: $10,092.61, Epsilon: 0.3906\n",
      "Episode 55/100, Reward: 171.3862, Portfolio: $10,171.39, Epsilon: 0.3888\n",
      "Episode 56/100, Reward: 141.5532, Portfolio: $10,141.55, Epsilon: 0.3871\n",
      "Episode 57/100, Reward: 173.6329, Portfolio: $10,173.63, Epsilon: 0.3853\n",
      "Episode 58/100, Reward: 141.5715, Portfolio: $10,141.57, Epsilon: 0.3835\n",
      "Episode 59/100, Reward: 110.4984, Portfolio: $10,110.50, Epsilon: 0.3818\n",
      "Episode 60/100, Reward: 155.5076, Portfolio: $10,155.51, Epsilon: 0.3800\n",
      "Models saved successfully\n",
      "Episode 61/100, Reward: 206.7296, Portfolio: $10,206.73, Epsilon: 0.3783\n",
      "Episode 62/100, Reward: 187.0821, Portfolio: $10,187.08, Epsilon: 0.3765\n",
      "Episode 63/100, Reward: 169.9190, Portfolio: $10,169.92, Epsilon: 0.3748\n",
      "Episode 64/100, Reward: 137.6475, Portfolio: $10,137.65, Epsilon: 0.3731\n",
      "Episode 65/100, Reward: 93.1253, Portfolio: $10,093.13, Epsilon: 0.3714\n",
      "Episode 66/100, Reward: 188.8535, Portfolio: $10,188.85, Epsilon: 0.3697\n",
      "Episode 67/100, Reward: 152.1668, Portfolio: $10,152.17, Epsilon: 0.3680\n",
      "Episode 68/100, Reward: 163.1484, Portfolio: $10,163.15, Epsilon: 0.3663\n",
      "Episode 69/100, Reward: 164.4277, Portfolio: $10,164.43, Epsilon: 0.3646\n",
      "Episode 70/100, Reward: 78.6235, Portfolio: $10,078.62, Epsilon: 0.3629\n",
      "Models saved successfully\n",
      "Episode 71/100, Reward: 168.7371, Portfolio: $10,168.74, Epsilon: 0.3613\n",
      "Episode 72/100, Reward: 88.9902, Portfolio: $10,088.99, Epsilon: 0.3596\n",
      "Episode 73/100, Reward: 105.4795, Portfolio: $10,105.48, Epsilon: 0.3580\n",
      "Episode 74/100, Reward: 235.1550, Portfolio: $10,235.15, Epsilon: 0.3563\n",
      "Episode 75/100, Reward: 254.7931, Portfolio: $10,254.79, Epsilon: 0.3547\n",
      "Episode 76/100, Reward: 185.3067, Portfolio: $10,185.31, Epsilon: 0.3531\n",
      "Episode 77/100, Reward: 184.8871, Portfolio: $10,184.89, Epsilon: 0.3515\n",
      "Episode 78/100, Reward: 126.2492, Portfolio: $10,126.25, Epsilon: 0.3498\n",
      "Episode 79/100, Reward: 89.9199, Portfolio: $10,089.92, Epsilon: 0.3482\n",
      "Episode 80/100, Reward: 137.8278, Portfolio: $10,137.83, Epsilon: 0.3466\n",
      "Models saved successfully\n",
      "Episode 81/100, Reward: 169.3021, Portfolio: $10,169.30, Epsilon: 0.3451\n",
      "Episode 82/100, Reward: 166.9505, Portfolio: $10,166.95, Epsilon: 0.3435\n",
      "Episode 83/100, Reward: 85.0638, Portfolio: $10,085.06, Epsilon: 0.3419\n",
      "Episode 84/100, Reward: 181.7127, Portfolio: $10,181.71, Epsilon: 0.3403\n",
      "Episode 85/100, Reward: 227.2529, Portfolio: $10,227.25, Epsilon: 0.3388\n",
      "Episode 86/100, Reward: 96.2377, Portfolio: $10,096.24, Epsilon: 0.3372\n",
      "Episode 87/100, Reward: 214.8133, Portfolio: $10,214.81, Epsilon: 0.3357\n",
      "Episode 88/100, Reward: 178.2413, Portfolio: $10,178.24, Epsilon: 0.3341\n",
      "Episode 89/100, Reward: 127.0337, Portfolio: $10,127.03, Epsilon: 0.3326\n",
      "Episode 90/100, Reward: 143.6210, Portfolio: $10,143.62, Epsilon: 0.3311\n",
      "Models saved successfully\n",
      "Episode 91/100, Reward: 188.5503, Portfolio: $10,188.55, Epsilon: 0.3296\n",
      "Episode 92/100, Reward: 257.3232, Portfolio: $10,257.32, Epsilon: 0.3280\n",
      "Episode 93/100, Reward: 75.9742, Portfolio: $10,075.97, Epsilon: 0.3265\n",
      "Episode 94/100, Reward: 120.9961, Portfolio: $10,121.00, Epsilon: 0.3250\n",
      "Episode 95/100, Reward: 231.7537, Portfolio: $10,231.75, Epsilon: 0.3236\n",
      "Episode 96/100, Reward: 162.3604, Portfolio: $10,162.36, Epsilon: 0.3221\n",
      "Episode 97/100, Reward: 204.5393, Portfolio: $10,204.54, Epsilon: 0.3206\n",
      "Episode 98/100, Reward: 182.5432, Portfolio: $10,182.54, Epsilon: 0.3191\n",
      "Episode 99/100, Reward: 202.3950, Portfolio: $10,202.39, Epsilon: 0.3177\n",
      "Episode 100/100, Reward: 200.0154, Portfolio: $10,200.02, Epsilon: 0.3162\n",
      "Models saved successfully\n",
      "\n",
      "Evaluating on test data...\n",
      "\n",
      "==================================================\n",
      "Initial Balance: $10,000.00\n",
      "Final Portfolio Value: $10,015.64\n",
      "Total Return: 0.16%\n",
      "Total Trades Executed: 24\n",
      "Grid Levels: [103867.26 103959.52 104051.77 104144.03 104236.28 104328.53 104420.79\n",
      " 104513.04 104605.3  104697.55 104789.81 104882.06 104974.31 105066.57\n",
      " 105158.82 105251.08 105343.33 105435.59 105527.84]\n",
      "=== Iteration 5 ===\n",
      "Training LSTM Price Predictor...\n",
      "loaded 95962.83333333333\n",
      "\n",
      "Training DQN Agent...\n",
      "Episode 1/100, Reward: 134.3291, Portfolio: $10,134.33, Epsilon: 0.4983\n",
      "Episode 2/100, Reward: 173.6020, Portfolio: $10,173.60, Epsilon: 0.4961\n",
      "Episode 3/100, Reward: 132.0234, Portfolio: $10,132.02, Epsilon: 0.4938\n",
      "Episode 4/100, Reward: 146.8250, Portfolio: $10,146.83, Epsilon: 0.4915\n",
      "Episode 5/100, Reward: 184.4366, Portfolio: $10,184.44, Epsilon: 0.4893\n",
      "Episode 6/100, Reward: 203.3808, Portfolio: $10,203.38, Epsilon: 0.4870\n",
      "Episode 7/100, Reward: 158.9263, Portfolio: $10,158.93, Epsilon: 0.4848\n",
      "Episode 8/100, Reward: 185.4308, Portfolio: $10,185.43, Epsilon: 0.4826\n",
      "Episode 9/100, Reward: 184.2425, Portfolio: $10,184.24, Epsilon: 0.4804\n",
      "Episode 10/100, Reward: 141.9236, Portfolio: $10,141.92, Epsilon: 0.4782\n",
      "Models saved successfully\n",
      "Episode 11/100, Reward: 175.8338, Portfolio: $10,175.83, Epsilon: 0.4760\n",
      "Episode 12/100, Reward: 167.1426, Portfolio: $10,167.14, Epsilon: 0.4738\n",
      "Episode 13/100, Reward: 4.2613, Portfolio: $10,004.26, Epsilon: 0.4716\n",
      "Episode 14/100, Reward: 97.7530, Portfolio: $10,097.75, Epsilon: 0.4694\n",
      "Episode 15/100, Reward: 127.6596, Portfolio: $10,127.66, Epsilon: 0.4673\n",
      "Episode 16/100, Reward: 175.8713, Portfolio: $10,175.87, Epsilon: 0.4652\n",
      "Episode 17/100, Reward: 128.3244, Portfolio: $10,128.32, Epsilon: 0.4630\n",
      "Episode 18/100, Reward: 179.9302, Portfolio: $10,179.93, Epsilon: 0.4609\n",
      "Episode 19/100, Reward: 82.9411, Portfolio: $10,082.94, Epsilon: 0.4588\n",
      "Episode 20/100, Reward: 53.9991, Portfolio: $10,054.00, Epsilon: 0.4567\n",
      "Models saved successfully\n",
      "Episode 21/100, Reward: 241.4755, Portfolio: $10,241.48, Epsilon: 0.4546\n",
      "Episode 22/100, Reward: 167.6870, Portfolio: $10,167.69, Epsilon: 0.4525\n",
      "Episode 23/100, Reward: 65.4303, Portfolio: $10,065.43, Epsilon: 0.4504\n",
      "Episode 24/100, Reward: 136.4092, Portfolio: $10,136.41, Epsilon: 0.4484\n",
      "Episode 25/100, Reward: 141.8356, Portfolio: $10,141.84, Epsilon: 0.4463\n",
      "Episode 26/100, Reward: 181.6040, Portfolio: $10,181.60, Epsilon: 0.4443\n",
      "Episode 27/100, Reward: 134.3303, Portfolio: $10,134.33, Epsilon: 0.4422\n",
      "Episode 28/100, Reward: 151.0015, Portfolio: $10,151.00, Epsilon: 0.4402\n",
      "Episode 29/100, Reward: 220.1716, Portfolio: $10,220.17, Epsilon: 0.4382\n",
      "Episode 30/100, Reward: 103.2586, Portfolio: $10,103.26, Epsilon: 0.4362\n",
      "Models saved successfully\n",
      "Episode 31/100, Reward: 129.7861, Portfolio: $10,129.79, Epsilon: 0.4342\n",
      "Episode 32/100, Reward: 99.6583, Portfolio: $10,099.66, Epsilon: 0.4322\n",
      "Episode 33/100, Reward: 153.0207, Portfolio: $10,153.02, Epsilon: 0.4302\n",
      "Episode 34/100, Reward: 141.7723, Portfolio: $10,141.77, Epsilon: 0.4282\n",
      "Episode 35/100, Reward: 115.7228, Portfolio: $10,115.72, Epsilon: 0.4263\n",
      "Episode 36/100, Reward: 184.4322, Portfolio: $10,184.43, Epsilon: 0.4243\n",
      "Episode 37/100, Reward: 296.5211, Portfolio: $10,296.52, Epsilon: 0.4224\n",
      "Episode 38/100, Reward: 83.4985, Portfolio: $10,083.50, Epsilon: 0.4204\n",
      "Episode 39/100, Reward: 124.4625, Portfolio: $10,124.46, Epsilon: 0.4185\n",
      "Episode 40/100, Reward: 246.4543, Portfolio: $10,246.45, Epsilon: 0.4166\n",
      "Models saved successfully\n",
      "Episode 41/100, Reward: 140.4483, Portfolio: $10,140.45, Epsilon: 0.4147\n",
      "Episode 42/100, Reward: 234.9908, Portfolio: $10,234.99, Epsilon: 0.4128\n",
      "Episode 43/100, Reward: 192.1054, Portfolio: $10,192.11, Epsilon: 0.4109\n",
      "Episode 44/100, Reward: 98.7613, Portfolio: $10,098.76, Epsilon: 0.4090\n",
      "Episode 45/100, Reward: 125.0862, Portfolio: $10,125.09, Epsilon: 0.4071\n",
      "Episode 46/100, Reward: 108.8471, Portfolio: $10,108.85, Epsilon: 0.4053\n",
      "Episode 47/100, Reward: 115.5232, Portfolio: $10,115.52, Epsilon: 0.4034\n",
      "Episode 48/100, Reward: 98.9195, Portfolio: $10,098.92, Epsilon: 0.4015\n",
      "Episode 49/100, Reward: 126.2379, Portfolio: $10,126.24, Epsilon: 0.3997\n",
      "Episode 50/100, Reward: 138.6201, Portfolio: $10,138.62, Epsilon: 0.3979\n",
      "Models saved successfully\n",
      "Episode 51/100, Reward: 197.5056, Portfolio: $10,197.51, Epsilon: 0.3961\n",
      "Episode 52/100, Reward: 175.3954, Portfolio: $10,175.40, Epsilon: 0.3942\n",
      "Episode 53/100, Reward: 156.1459, Portfolio: $10,156.15, Epsilon: 0.3924\n",
      "Episode 54/100, Reward: 123.3783, Portfolio: $10,123.38, Epsilon: 0.3906\n",
      "Episode 55/100, Reward: 109.9034, Portfolio: $10,109.90, Epsilon: 0.3888\n",
      "Episode 56/100, Reward: 121.8895, Portfolio: $10,121.89, Epsilon: 0.3871\n",
      "Episode 57/100, Reward: 121.1155, Portfolio: $10,121.12, Epsilon: 0.3853\n",
      "Episode 58/100, Reward: 122.3285, Portfolio: $10,122.33, Epsilon: 0.3835\n",
      "Episode 59/100, Reward: 199.1183, Portfolio: $10,199.12, Epsilon: 0.3818\n",
      "Episode 60/100, Reward: 169.5452, Portfolio: $10,169.55, Epsilon: 0.3800\n",
      "Models saved successfully\n",
      "Episode 61/100, Reward: 200.1930, Portfolio: $10,200.19, Epsilon: 0.3783\n",
      "Episode 62/100, Reward: 152.3650, Portfolio: $10,152.37, Epsilon: 0.3765\n",
      "Episode 63/100, Reward: 189.8388, Portfolio: $10,189.84, Epsilon: 0.3748\n",
      "Episode 64/100, Reward: 100.0037, Portfolio: $10,100.00, Epsilon: 0.3731\n",
      "Episode 65/100, Reward: 59.6185, Portfolio: $10,059.62, Epsilon: 0.3714\n",
      "Episode 66/100, Reward: 117.3971, Portfolio: $10,117.40, Epsilon: 0.3697\n",
      "Episode 67/100, Reward: 132.7874, Portfolio: $10,132.79, Epsilon: 0.3680\n",
      "Episode 68/100, Reward: 230.4261, Portfolio: $10,230.43, Epsilon: 0.3663\n",
      "Episode 69/100, Reward: 116.2889, Portfolio: $10,116.29, Epsilon: 0.3646\n",
      "Episode 70/100, Reward: 194.9344, Portfolio: $10,194.93, Epsilon: 0.3629\n",
      "Models saved successfully\n",
      "Episode 71/100, Reward: 157.3464, Portfolio: $10,157.35, Epsilon: 0.3613\n",
      "Episode 72/100, Reward: 129.9257, Portfolio: $10,129.93, Epsilon: 0.3596\n",
      "Episode 73/100, Reward: 88.4405, Portfolio: $10,088.44, Epsilon: 0.3580\n",
      "Episode 74/100, Reward: 125.4899, Portfolio: $10,125.49, Epsilon: 0.3563\n",
      "Episode 75/100, Reward: 59.3551, Portfolio: $10,059.36, Epsilon: 0.3547\n",
      "Episode 76/100, Reward: 129.5457, Portfolio: $10,129.55, Epsilon: 0.3531\n",
      "Episode 77/100, Reward: 123.2519, Portfolio: $10,123.25, Epsilon: 0.3515\n",
      "Episode 78/100, Reward: 181.2615, Portfolio: $10,181.26, Epsilon: 0.3498\n",
      "Episode 79/100, Reward: 101.1666, Portfolio: $10,101.17, Epsilon: 0.3482\n",
      "Episode 80/100, Reward: 209.5307, Portfolio: $10,209.53, Epsilon: 0.3466\n",
      "Models saved successfully\n",
      "Episode 81/100, Reward: 154.3946, Portfolio: $10,154.39, Epsilon: 0.3451\n",
      "Episode 82/100, Reward: 126.4671, Portfolio: $10,126.47, Epsilon: 0.3435\n",
      "Episode 83/100, Reward: 164.1858, Portfolio: $10,164.19, Epsilon: 0.3419\n",
      "Episode 84/100, Reward: 123.6933, Portfolio: $10,123.69, Epsilon: 0.3403\n",
      "Episode 85/100, Reward: 165.6238, Portfolio: $10,165.62, Epsilon: 0.3388\n",
      "Episode 86/100, Reward: 110.7701, Portfolio: $10,110.77, Epsilon: 0.3372\n",
      "Episode 87/100, Reward: 106.3240, Portfolio: $10,106.32, Epsilon: 0.3357\n",
      "Episode 88/100, Reward: 155.9250, Portfolio: $10,155.93, Epsilon: 0.3341\n",
      "Episode 89/100, Reward: 141.4498, Portfolio: $10,141.45, Epsilon: 0.3326\n",
      "Episode 90/100, Reward: 167.0360, Portfolio: $10,167.04, Epsilon: 0.3311\n",
      "Models saved successfully\n",
      "Episode 91/100, Reward: 199.2357, Portfolio: $10,199.24, Epsilon: 0.3296\n",
      "Episode 92/100, Reward: 190.2626, Portfolio: $10,190.26, Epsilon: 0.3280\n",
      "Episode 93/100, Reward: 197.7376, Portfolio: $10,197.74, Epsilon: 0.3265\n",
      "Episode 94/100, Reward: 161.4070, Portfolio: $10,161.41, Epsilon: 0.3250\n",
      "Episode 95/100, Reward: 146.0092, Portfolio: $10,146.01, Epsilon: 0.3236\n",
      "Episode 96/100, Reward: 87.7951, Portfolio: $10,087.80, Epsilon: 0.3221\n",
      "Episode 97/100, Reward: 131.9424, Portfolio: $10,131.94, Epsilon: 0.3206\n",
      "Episode 98/100, Reward: 103.2721, Portfolio: $10,103.27, Epsilon: 0.3191\n",
      "Episode 99/100, Reward: 96.1574, Portfolio: $10,096.16, Epsilon: 0.3177\n",
      "Episode 100/100, Reward: 149.9791, Portfolio: $10,149.98, Epsilon: 0.3162\n",
      "Models saved successfully\n",
      "\n",
      "Evaluating on test data...\n",
      "\n",
      "==================================================\n",
      "Initial Balance: $10,000.00\n",
      "Final Portfolio Value: $10,022.88\n",
      "Total Return: 0.23%\n",
      "Total Trades Executed: 56\n",
      "Grid Levels: [103985.54 104071.57 104157.6  104243.63 104329.66 104415.68 104501.71\n",
      " 104587.74 104673.77 104759.8  104845.83 104931.86 105017.89 105103.92\n",
      " 105189.95 105275.98 105362.01 105448.04 105534.07]\n",
      "=== Iteration 6 ===\n",
      "Training LSTM Price Predictor...\n",
      "loaded 95962.83333333333\n",
      "\n",
      "Training DQN Agent...\n",
      "Episode 1/100, Reward: 125.2151, Portfolio: $10,125.22, Epsilon: 0.4983\n",
      "Episode 2/100, Reward: 126.3428, Portfolio: $10,126.34, Epsilon: 0.4961\n",
      "Episode 3/100, Reward: 102.5345, Portfolio: $10,102.53, Epsilon: 0.4938\n",
      "Episode 4/100, Reward: 166.1529, Portfolio: $10,166.15, Epsilon: 0.4915\n",
      "Episode 5/100, Reward: 143.3016, Portfolio: $10,143.30, Epsilon: 0.4893\n",
      "Episode 6/100, Reward: 172.9759, Portfolio: $10,172.98, Epsilon: 0.4870\n",
      "Episode 7/100, Reward: 107.5107, Portfolio: $10,107.51, Epsilon: 0.4848\n",
      "Episode 8/100, Reward: 67.6056, Portfolio: $10,067.61, Epsilon: 0.4826\n",
      "Episode 9/100, Reward: 159.5878, Portfolio: $10,159.59, Epsilon: 0.4804\n",
      "Episode 10/100, Reward: 160.8601, Portfolio: $10,160.86, Epsilon: 0.4782\n",
      "Models saved successfully\n",
      "Episode 11/100, Reward: 200.5487, Portfolio: $10,200.55, Epsilon: 0.4760\n",
      "Episode 12/100, Reward: 166.0336, Portfolio: $10,166.03, Epsilon: 0.4738\n",
      "Episode 13/100, Reward: 197.3367, Portfolio: $10,197.34, Epsilon: 0.4716\n",
      "Episode 14/100, Reward: 35.6713, Portfolio: $10,035.67, Epsilon: 0.4694\n",
      "Episode 15/100, Reward: 173.7225, Portfolio: $10,173.72, Epsilon: 0.4673\n",
      "Episode 16/100, Reward: 86.2986, Portfolio: $10,086.30, Epsilon: 0.4652\n",
      "Episode 17/100, Reward: 148.1702, Portfolio: $10,148.17, Epsilon: 0.4630\n",
      "Episode 18/100, Reward: 189.9382, Portfolio: $10,189.94, Epsilon: 0.4609\n",
      "Episode 19/100, Reward: 160.6492, Portfolio: $10,160.65, Epsilon: 0.4588\n",
      "Episode 20/100, Reward: 155.7689, Portfolio: $10,155.77, Epsilon: 0.4567\n",
      "Models saved successfully\n",
      "Episode 21/100, Reward: 55.7022, Portfolio: $10,055.70, Epsilon: 0.4546\n",
      "Episode 22/100, Reward: 107.3501, Portfolio: $10,107.35, Epsilon: 0.4525\n",
      "Episode 23/100, Reward: 144.7405, Portfolio: $10,144.74, Epsilon: 0.4504\n",
      "Episode 24/100, Reward: 121.7628, Portfolio: $10,121.76, Epsilon: 0.4484\n",
      "Episode 25/100, Reward: 170.7799, Portfolio: $10,170.78, Epsilon: 0.4463\n",
      "Episode 26/100, Reward: 123.6249, Portfolio: $10,123.62, Epsilon: 0.4443\n",
      "Episode 27/100, Reward: 171.7158, Portfolio: $10,171.72, Epsilon: 0.4422\n",
      "Episode 28/100, Reward: 146.1681, Portfolio: $10,146.17, Epsilon: 0.4402\n",
      "Episode 29/100, Reward: 117.5765, Portfolio: $10,117.58, Epsilon: 0.4382\n",
      "Episode 30/100, Reward: 190.0853, Portfolio: $10,190.09, Epsilon: 0.4362\n",
      "Models saved successfully\n",
      "Episode 31/100, Reward: 179.8861, Portfolio: $10,179.89, Epsilon: 0.4342\n",
      "Episode 32/100, Reward: 97.2752, Portfolio: $10,097.28, Epsilon: 0.4322\n",
      "Episode 33/100, Reward: 97.3084, Portfolio: $10,097.31, Epsilon: 0.4302\n",
      "Episode 34/100, Reward: 73.5664, Portfolio: $10,073.57, Epsilon: 0.4282\n",
      "Episode 35/100, Reward: 99.2331, Portfolio: $10,099.23, Epsilon: 0.4263\n",
      "Episode 36/100, Reward: 124.4806, Portfolio: $10,124.48, Epsilon: 0.4243\n",
      "Episode 37/100, Reward: 73.5995, Portfolio: $10,073.60, Epsilon: 0.4224\n",
      "Episode 38/100, Reward: 123.2288, Portfolio: $10,123.23, Epsilon: 0.4204\n",
      "Episode 39/100, Reward: 179.8483, Portfolio: $10,179.85, Epsilon: 0.4185\n",
      "Episode 40/100, Reward: 161.5494, Portfolio: $10,161.55, Epsilon: 0.4166\n",
      "Models saved successfully\n",
      "Episode 41/100, Reward: 261.6401, Portfolio: $10,261.64, Epsilon: 0.4147\n",
      "Episode 42/100, Reward: 187.9796, Portfolio: $10,187.98, Epsilon: 0.4128\n",
      "Episode 43/100, Reward: 90.7162, Portfolio: $10,090.72, Epsilon: 0.4109\n",
      "Episode 44/100, Reward: 88.3781, Portfolio: $10,088.38, Epsilon: 0.4090\n",
      "Episode 45/100, Reward: 182.3281, Portfolio: $10,182.33, Epsilon: 0.4071\n",
      "Episode 46/100, Reward: 119.7913, Portfolio: $10,119.79, Epsilon: 0.4053\n",
      "Episode 47/100, Reward: 112.6573, Portfolio: $10,112.66, Epsilon: 0.4034\n",
      "Episode 48/100, Reward: 151.2722, Portfolio: $10,151.27, Epsilon: 0.4015\n",
      "Episode 49/100, Reward: 172.1252, Portfolio: $10,172.13, Epsilon: 0.3997\n",
      "Episode 50/100, Reward: 163.1909, Portfolio: $10,163.19, Epsilon: 0.3979\n",
      "Models saved successfully\n",
      "Episode 51/100, Reward: 119.2772, Portfolio: $10,119.28, Epsilon: 0.3961\n",
      "Episode 52/100, Reward: 34.4199, Portfolio: $10,034.42, Epsilon: 0.3942\n",
      "Episode 53/100, Reward: 88.4473, Portfolio: $10,088.45, Epsilon: 0.3924\n",
      "Episode 54/100, Reward: 121.3799, Portfolio: $10,121.38, Epsilon: 0.3906\n",
      "Episode 55/100, Reward: 148.9065, Portfolio: $10,148.91, Epsilon: 0.3888\n",
      "Episode 56/100, Reward: 81.4523, Portfolio: $10,081.45, Epsilon: 0.3871\n",
      "Episode 57/100, Reward: 57.0635, Portfolio: $10,057.06, Epsilon: 0.3853\n",
      "Episode 58/100, Reward: 159.6472, Portfolio: $10,159.65, Epsilon: 0.3835\n",
      "Episode 59/100, Reward: 95.2640, Portfolio: $10,095.26, Epsilon: 0.3818\n",
      "Episode 60/100, Reward: 48.7400, Portfolio: $10,048.74, Epsilon: 0.3800\n",
      "Models saved successfully\n",
      "Episode 61/100, Reward: 82.9715, Portfolio: $10,082.97, Epsilon: 0.3783\n",
      "Episode 62/100, Reward: 135.5389, Portfolio: $10,135.54, Epsilon: 0.3765\n",
      "Episode 63/100, Reward: 96.1197, Portfolio: $10,096.12, Epsilon: 0.3748\n",
      "Episode 64/100, Reward: 100.2858, Portfolio: $10,100.29, Epsilon: 0.3731\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "def train_model(csv):\n",
    "    \n",
    "    processor = DataProcessor(csv)\n",
    "    scaled_data, raw_prices = processor.preprocess()\n",
    "    X, y = processor.create_sequences(scaled_data, raw_prices)\n",
    "    joblib.dump(processor.scaler, 'min_max_scaler.joblib')\n",
    "    # Split data\n",
    "    split = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:split], X[split:]\n",
    "    y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "    # 2. Train LSTM Price Predictor\n",
    "    print(\"Training LSTM Price Predictor...\")\n",
    "    lstm_model = train_lstm(X_train, y_train, input_size=X.shape[2], epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "    lstm_model.eval()\n",
    "    \n",
    "    # 3. Prepare Trading Environment\n",
    "    grids = create_grids(raw_prices)\n",
    "    env = AdaptiveTradingEnvironment(\n",
    "    scaled_data, \n",
    "    raw_prices, \n",
    "    lstm_model, \n",
    "    initial_grids=grids,\n",
    "    recalibration_interval=1000,  # Recalculate every 200 steps\n",
    "    lookback_window=500         # Use last 500 prices for recalculation\n",
    "    )\n",
    "    # 4. Initialize DQN Agent\n",
    "    state_size = env.get_state().shape[0]\n",
    "    action_size = 3  # hold, buy, sell\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    agent.epsilon = 0.5\n",
    "    if os.path.exists(DDDQN_PATH):\n",
    "        agent.load(DDDQN_PATH)\n",
    "    # 5. Training Loop\n",
    "    print(\"\\nTraining DQN Agent...\")\n",
    "    portfolio_history = []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_portfolio = [INITIAL_BALANCE]\n",
    "        \n",
    "        while not env.done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, portfolio_value = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            episode_portfolio.append(portfolio_value)\n",
    "            \n",
    "            if len(agent.memory) > BATCH_SIZE:\n",
    "                agent.replay(BATCH_SIZE)\n",
    "        \n",
    "        portfolio_history.append(episode_portfolio[-1])\n",
    "        print(f\"Episode {e+1}/{EPISODES}, Reward: {total_reward:.4f}, Portfolio: ${episode_portfolio[-1]:,.2f}, Epsilon: {agent.epsilon:.4f}\")\n",
    "        if (e+1) % 10 == 0:\n",
    "            agent.save(DDDQN_PATH)\n",
    "            print(\"Models saved successfully\")\n",
    "    # 6. Evaluation\n",
    "    print(\"\\nEvaluating on test data...\")\n",
    "    test_env = TradingEnvironment(scaled_data[split:], raw_prices[split:], lstm_model, grids)\n",
    "    state = test_env.reset()\n",
    "    portfolio_values = []\n",
    "    trade_log = []\n",
    "\n",
    "    while not test_env.done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, portfolio_value = test_env.step(action)\n",
    "        state = next_state\n",
    "        portfolio_values.append(portfolio_value)\n",
    "        \n",
    "        if test_env.trades and test_env.trades[-1][0] in ['buy', 'sell']:\n",
    "            trade_log.append(test_env.trades[-1])\n",
    "\n",
    "    # 7. Results\n",
    "    final_value = test_env.balance + test_env.btc_held * test_env.prices[-1]\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Initial Balance: ${INITIAL_BALANCE:,.2f}\")\n",
    "    print(f\"Final Portfolio Value: ${final_value:,.2f}\")\n",
    "    print(f\"Total Return: {((final_value - INITIAL_BALANCE) / INITIAL_BALANCE)*100:.2f}%\")\n",
    "    print(f\"Total Trades Executed: {len(test_env.trades)}\")\n",
    "    print(f\"Grid Levels: {np.around(grids, 2)}\")\n",
    "\n",
    "    # # Plot results\n",
    "    # plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # # Price and grid levels\n",
    "    # plt.subplot(3, 1, 1)\n",
    "    # plt.plot(test_env.prices, label='Price')\n",
    "    # for grid in grids:\n",
    "    #     plt.axhline(y=grid, color='gray', linestyle='--', alpha=0.3)\n",
    "    # plt.title('Price with Grid Levels')\n",
    "    # plt.legend()\n",
    "\n",
    "    # # Portfolio value\n",
    "    # plt.subplot(3, 1, 2)\n",
    "    # plt.plot(portfolio_values)\n",
    "    # plt.title('Portfolio Value')\n",
    "    # plt.xlabel('Time Step')\n",
    "    # plt.ylabel('Value (USD)')\n",
    "\n",
    "    # # Trade visualization\n",
    "    # plt.subplot(3, 1, 3)\n",
    "    # buys = [t[1] for t in trade_log if t[0] == 'buy']\n",
    "    # sells = [t[1] for t in trade_log if t[0] == 'sell']\n",
    "    # plt.plot(test_env.prices, label='Price')\n",
    "    # plt.scatter([i for i, t in enumerate(trade_log) if t[0]=='buy'], buys, \n",
    "    #             color='green', marker='^', alpha=0.7, label='Buy')\n",
    "    # plt.scatter([i for i, t in enumerate(trade_log) if t[0]=='sell'], sells, \n",
    "    #             color='red', marker='v', alpha=0.7, label='Sell')\n",
    "    # plt.title('Trading Signals')\n",
    "    # plt.legend()\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig('trading_results.png')\n",
    "    # plt.show()\n",
    "\n",
    "    # plot_grid_history(env)\n",
    "    # # Save models\n",
    "    # torch.save(lstm_model.state_dict(), LSTM_PATH)\n",
    "    # agent.save(DDDQN_PATH)\n",
    "    # print(\"Models saved successfully\")\n",
    "\n",
    "# 1. Data Preparation\n",
    "def dataCreation():\n",
    "    df_live = get_binance_data(symbol=TICKER, interval=Client.KLINE_INTERVAL_1MINUTE, lookback='1000')\n",
    "    if df_live.empty:\n",
    "        raise Exception(\"No data retrieved; skipping model training.\")\n",
    "    temp_csv = 'live_temp.csv'\n",
    "    df_live.to_csv(temp_csv, index=False)\n",
    "    return temp_csv\n",
    "\n",
    "for i in range(1000):\n",
    "    try:\n",
    "        print(f\"=== Iteration {i+1} ===\")\n",
    "        temp_csv = dataCreation()\n",
    "        train_model(temp_csv)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in loop iteration {i+1}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d635e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import schedule\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from binance.client import Client\n",
    "from binance.exceptions import BinanceAPIException, BinanceOrderException\n",
    "from collections import deque\n",
    "import joblib # For loading scaler  \n",
    "\n",
    "# --- Configuration (mostly from your Cell 1) ---\n",
    "TICKER = 'BTCUSDT'\n",
    "SEQ_LENGTH = 60\n",
    "PREDICTION_STEPS = 1 # Not directly used in live state construction if LSTM predicts next price\n",
    "INITIAL_BALANCE_REF = 10000.0 # Reference for normalization, actual balance managed live\n",
    "TRADE_FEE_RATE = 0.0001 # For local simulation if needed, actual fees by Binance\n",
    "MIN_TRADE_AMOUNT_USD = 10 # Minimum trade value in USD\n",
    "GRID_COUNT = 100\n",
    "LSTM_INPUT_FEATURES = 7 # open, high, low, close, volume, Returns, Volatility\n",
    "DQN_STATE_SIZE = 11    # 7 market features + norm_balance + norm_btc_value + grid_pos + pred/price\n",
    "ACTION_SIZE = 3        # Hold, Buy, Sell\n",
    "\n",
    "# Live trading parameters\n",
    "LIVE_RECALIBRATION_INTERVAL = 200 # Steps (minutes)\n",
    "LIVE_LOOKBACK_WINDOW_GRID = 500 # Steps (minutes) for grid recalc\n",
    "ORDER_TYPE = Client.ORDER_TYPE_MARKET\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device for live trading: {device}\")\n",
    "\n",
    "# --- Global State (Initialize once) ---\n",
    "# These will be initialized in start_live_trading_bot()\n",
    "binance_client = None\n",
    "lstm_predictor_live = None\n",
    "dqn_agent_live = None\n",
    "min_max_scaler_live = None\n",
    "\n",
    "# Portfolio and Grid State\n",
    "current_usdt_balance = 1000.0 # Example: Starting USDT for live test\n",
    "current_btc_held = 0.0\n",
    "current_grid_levels = np.array([])\n",
    "live_steps_since_recal = 0\n",
    "live_recent_prices_for_grid = deque(maxlen=LIVE_LOOKBACK_WINDOW_GRID)\n",
    "\n",
    "# --- Helper: Load your LSTMPredictor and DQN classes (copy from Cell 1) ---\n",
    "# class LSTMPredictor(nn.Module): ... (already defined in your notebook)\n",
    "# class DQN(nn.Module): ... (already defined in your notebook)\n",
    "# class DQNAgent: ... (already defined in your notebook, we only need its model for acting)\n",
    "# def create_grids(prices, num_grids=GRID_COUNT): ... (already defined)\n",
    "# def get_binance_data(...): ... (already defined)\n",
    "\n",
    "def initialize_live_trader():\n",
    "    global binance_client, lstm_predictor_live, dqn_agent_live, min_max_scaler_live\n",
    "    global current_grid_levels, current_usdt_balance, current_btc_held\n",
    "\n",
    "    # 1. Initialize Binance Client (Testnet)\n",
    "\n",
    "    api_key = 'OrR1dfYW9wa8pjjgfOrk5XhdICHZEwFvsKSCgmUgyGrFu7YMfvR2WBhbTMELu858'\n",
    "    api_secret = 'JOENNjZJ7PWeVp6GbQBMcBJC95VzjlASitaeSVol1jIzjmIpnLXUWsganegbQZVE'\n",
    "    if not api_key or not api_secret:\n",
    "        print(\"FATAL: Binance Testnet API keys not found in environment variables.\")\n",
    "        return False\n",
    "    binance_client = Client(api_key, api_secret, testnet=True)\n",
    "    try:\n",
    "        binance_client.ping()\n",
    "        print(\"Binance Testnet client initialized and connected.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to Binance Testnet: {e}\")\n",
    "        return False\n",
    "\n",
    "    # 2. Load Scaler\n",
    "    try:\n",
    "        min_max_scaler_live = joblib.load('min_max_scaler.joblib')\n",
    "        print(\"MinMaxScaler loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"FATAL: min_max_scaler.joblib not found. Train model first to generate it.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading scaler: {e}\")\n",
    "        return False\n",
    "\n",
    "    # 3. Load LSTM Predictor\n",
    "    lstm_predictor_live = LSTMPredictor(input_size=LSTM_INPUT_FEATURES).to(device)\n",
    "    try:\n",
    "        lstm_predictor_live.load_state_dict(torch.load(LSTM_PATH, map_location=device))\n",
    "        lstm_predictor_live.eval()\n",
    "        print(\"LSTM model loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"FATAL: lstm_predictor_fixed_grid2.pth not found.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading LSTM model: {e}\")\n",
    "        return False\n",
    "\n",
    "    # 4. Load DQN Agent's Model\n",
    "    # We need the DQNAgent class definition if not already available globally\n",
    "    # For simplicity, we'll load the state_dict into a DQN model instance\n",
    "    dqn_model_live = DQN(DQN_STATE_SIZE, ACTION_SIZE).to(device)\n",
    "    try:\n",
    "        # Assuming dueldqn_agent_fixed_grid.pth saves the model.state_dict()\n",
    "        dqn_model_live.load_state_dict(torch.load(DDDQN_PATH, map_location=device))\n",
    "        dqn_model_live.eval()\n",
    "        # Wrap it in a simple structure for the .act() method or use dqn_model_live directly\n",
    "        class TempDQNAgent:\n",
    "            def __init__(self, model):\n",
    "                self.model = model\n",
    "                self.action_size = model.fc3.out_features # Infer action_size\n",
    "                self.epsilon = 0.0 # No exploration in live trading\n",
    "\n",
    "            def act(self, state_tensor): # Expects a tensor\n",
    "                if np.random.rand() <= self.epsilon:\n",
    "                    return random.randrange(self.action_size)\n",
    "                with torch.no_grad():\n",
    "                    q_values = self.model(state_tensor)\n",
    "                return torch.argmax(q_values).item()\n",
    "\n",
    "        dqn_agent_live = TempDQNAgent(dqn_model_live)\n",
    "        print(\"DQN model loaded successfully.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"FATAL: dueldqn_agent_fixed_grid.pth not found.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading DQN model: {e}\")\n",
    "        return False\n",
    "\n",
    "    # 5. Initialize Grids (fetch some recent data to base them on)\n",
    "    print(\"Fetching initial data for grid setup...\")\n",
    "    initial_price_data = get_binance_data(symbol=TICKER, interval=Client.KLINE_INTERVAL_1MINUTE, lookback='200')\n",
    "    if not initial_price_data.empty and len(initial_price_data) > 20:\n",
    "        current_grid_levels = create_grids(initial_price_data['close'].values, GRID_COUNT)\n",
    "        print(f\"Initial grid levels created: {np.around(current_grid_levels[:5], 2)}... (first 5)\")\n",
    "        # Populate recent prices for adaptive grid\n",
    "        for price in initial_price_data['close'].values[-(LIVE_LOOKBACK_WINDOW_GRID//2):]: # prime part of it\n",
    "            live_recent_prices_for_grid.append(price)\n",
    "    else:\n",
    "        print(\"Could not fetch enough initial data for grid setup. Using default wide grids.\")\n",
    "        # Fallback grids if data fetch fails, adjust as needed\n",
    "        avg_price_guess = 60000\n",
    "        current_grid_levels = np.linspace(avg_price_guess * 0.8, avg_price_guess * 1.2, GRID_COUNT + 1)[1:-1]\n",
    "\n",
    "    # Fetch initial balance from Testnet (optional, or use hardcoded start)\n",
    "    # For this example, we use the hardcoded current_usdt_balance and current_btc_held\n",
    "    print(f\"Initial live trading balance: USDT: {current_usdt_balance}, BTC: {current_btc_held}\")\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def recalculate_grids_live(prices_list):\n",
    "    \"\"\"Recalculate grids based on recent price window for live trading.\"\"\"\n",
    "    if not prices_list or len(prices_list) < 50: # Need enough data\n",
    "        return current_grid_levels # Return existing if not enough data\n",
    "    \n",
    "    recent_prices = np.array(prices_list)\n",
    "    low = np.percentile(recent_prices, 10)\n",
    "    high = np.percentile(recent_prices, 90)\n",
    "    \n",
    "    if high <= low: # Safety check\n",
    "        print(\"Warning: Grid recalculation - high <= low. Widening range.\")\n",
    "        high = low * 1.05 # Ensure some spread\n",
    "        if high == low : high = low + 100 # if low is 0 or very small\n",
    "\n",
    "    new_grids = np.linspace(low, high, num=GRID_COUNT + 1)[1:-1]\n",
    "    if len(new_grids) == 0: # another safety\n",
    "        print(\"Warning: Grid recalculation resulted in empty grids. Keeping old ones.\")\n",
    "        return current_grid_levels\n",
    "    return new_grids\n",
    "\n",
    "\n",
    "def run_live_trading_cycle():\n",
    "    global current_usdt_balance, current_btc_held, current_grid_levels\n",
    "    global live_steps_since_recal, live_recent_prices_for_grid\n",
    "    global lstm_predictor_live, dqn_agent_live, min_max_scaler_live, binance_client\n",
    "\n",
    "    print(f\"\\n[{time.ctime()}] --- Running Live Trading Cycle ---\")\n",
    "\n",
    "    # 1. Fetch Market Data\n",
    "    # Need SEQ_LENGTH for LSTM input + some buffer for feature calculation (e.g., rolling volatility)\n",
    "    # The number of features for scaling is 7: 'open', 'high', 'low', 'close', 'volume', 'Returns', 'Volatility'\n",
    "    required_klines = SEQ_LENGTH + 20 # 20 for rolling window of volatility\n",
    "    df_live_raw = get_binance_data(symbol=TICKER, interval=Client.KLINE_INTERVAL_1MINUTE, lookback=str(required_klines))\n",
    "\n",
    "    if df_live_raw.empty or len(df_live_raw) < required_klines:\n",
    "        print(\"Not enough live data fetched. Skipping cycle.\")\n",
    "        return\n",
    "\n",
    "    current_price = df_live_raw['close'].iloc[-1]\n",
    "    live_recent_prices_for_grid.append(current_price)\n",
    "    print(f\"Current Price ({TICKER}): {current_price:.2f}\")\n",
    "\n",
    "    # 2. Preprocess Data for LSTM\n",
    "    df_features = df_live_raw[['open', 'high', 'low', 'close', 'volume']].copy()\n",
    "    df_features['Returns'] = df_features['close'].pct_change()\n",
    "    df_features['Volatility'] = df_features['Returns'].rolling(window=20).std() # Match training\n",
    "    df_features.dropna(inplace=True) # Remove NaNs from rolling calculations\n",
    "\n",
    "    if len(df_features) < SEQ_LENGTH:\n",
    "        print(\"Not enough data after feature engineering. Skipping cycle.\")\n",
    "        return\n",
    "\n",
    "    # Select the exact features used for scaling during training\n",
    "    features_to_scale = df_features[['open', 'high', 'low', 'close', 'volume', 'Returns', 'Volatility']]\n",
    "    scaled_live_features = min_max_scaler_live.transform(features_to_scale)\n",
    "    \n",
    "    lstm_input_sequence = scaled_live_features[-SEQ_LENGTH:] # Shape: (SEQ_LENGTH, LSTM_INPUT_FEATURES)\n",
    "    lstm_input_tensor = torch.tensor(lstm_input_sequence[np.newaxis, ...], dtype=torch.float32).to(device)\n",
    "\n",
    "    # 3. Get LSTM Prediction\n",
    "    with torch.no_grad():\n",
    "        # Assuming LSTM predicts the next closing price (actual value, not scaled)\n",
    "        # This needs to match how LSTM was trained and how its output was used in env.get_state()\n",
    "        lstm_predicted_price = lstm_predictor_live(lstm_input_tensor).cpu().item()\n",
    "    \n",
    "    # The state uses prediction / price.\n",
    "    # If LSTM output is already a price, this is fine.\n",
    "    # If LSTM output is scaled, it needs to be inverse_transformed first.\n",
    "    # Your LSTMPredictor outputs a single value, assumed to be the price prediction.\n",
    "    normalized_lstm_prediction = lstm_predicted_price / current_price if current_price != 0 else 0\n",
    "    print(f\"LSTM Predicted Price: {lstm_predicted_price:.2f}, Normalized for state: {normalized_lstm_prediction:.4f}\")\n",
    "\n",
    "    # 4. Construct DQN State\n",
    "    # Last tick of scaled data for the state\n",
    "    current_scaled_market_data_tick = lstm_input_sequence[-1] \n",
    "    \n",
    "    # Grid position\n",
    "    if len(current_grid_levels) == 0: # Safety for uninitialized grids\n",
    "        grid_position_live = 0.5 # Default to middle\n",
    "    else:\n",
    "        grid_position_live = np.digitize(current_price, current_grid_levels) / len(current_grid_levels) # Normalized\n",
    "        grid_position_live = np.clip(grid_position_live, 0, 1) # Ensure it's within [0,1]\n",
    "\n",
    "\n",
    "    # Normalize balance and holdings for the state\n",
    "    # Using INITIAL_BALANCE_REF as the denominator for normalization consistency\n",
    "    norm_balance = current_usdt_balance / INITIAL_BALANCE_REF\n",
    "    norm_btc_value = (current_btc_held * current_price) / INITIAL_BALANCE_REF\n",
    "\n",
    "    live_state_np = np.array([\n",
    "        *current_scaled_market_data_tick,\n",
    "        norm_balance,\n",
    "        norm_btc_value,\n",
    "        grid_position_live,\n",
    "        normalized_lstm_prediction\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    if len(live_state_np) != DQN_STATE_SIZE:\n",
    "        print(f\"FATAL: Constructed state size {len(live_state_np)} != DQN_STATE_SIZE {DQN_STATE_SIZE}. Mismatch in features.\")\n",
    "        return\n",
    "\n",
    "    live_state_tensor = torch.tensor(live_state_np, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    # 5. Get DQN Action\n",
    "    action = dqn_agent_live.act(live_state_tensor) # 0: hold, 1: buy, 2: sell\n",
    "    action_map = {0: \"HOLD\", 1: \"BUY\", 2: \"SELL\"}\n",
    "    print(f\"DQN Action: {action_map[action]}\")\n",
    "\n",
    "    # 6. Execute Trade\n",
    "    if action == 1:  # Buy\n",
    "        # Logic adapted from your TradingEnvironment.step()\n",
    "        # Buy more when price is near lower grids (grid_position_live close to 0)\n",
    "        buy_multiplier = max(0.1, 1 - grid_position_live)\n",
    "        # Example: allocate a percentage of balance based on multiplier, e.g., 5% of balance * multiplier\n",
    "        # This is a simplification; your original code might have more complex sizing.\n",
    "        # Ensure this is a dynamic part of your strategy.\n",
    "        # Let's say we decide to use up to 10% of available USDT balance for a trade, scaled by multiplier\n",
    "        max_spend_usd = current_usdt_balance * 0.10 \n",
    "        amount_to_spend_usd = max_spend_usd * buy_multiplier\n",
    "        \n",
    "        if amount_to_spend_usd >= MIN_TRADE_AMOUNT_USD and current_usdt_balance >= amount_to_spend_usd :\n",
    "            print(f\"Attempting BUY: Spend {amount_to_spend_usd:.2f} USDT for {TICKER} at ~{current_price:.2f}\")\n",
    "            try:\n",
    "                order = binance_client.create_order( # Use create_order for real testnet trades\n",
    "                    symbol=TICKER,\n",
    "                    side=Client.SIDE_BUY,\n",
    "                    type=ORDER_TYPE,\n",
    "                    quoteOrderQty=f\"{amount_to_spend_usd:.8f}\") # Binance API needs string for precision\n",
    "                print(f\"BUY Test Order successful: {order}\")\n",
    "                # For real orders, you'd update balance after confirmation:\n",
    "                # current_usdt_balance -= actual_cost_from_order_fills\n",
    "                # current_btc_held += actual_btc_bought_from_order_fills\n",
    "            except BinanceAPIException as e:\n",
    "                print(f\"Binance API Exception (BUY): {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error executing BUY order: {e}\")\n",
    "        else:\n",
    "            print(f\"BUY condition not met: Spend amount {amount_to_spend_usd:.2f} < min OR insufficient balance.\")\n",
    "\n",
    "    elif action == 2:  # Sell\n",
    "        # Sell more when price is near upper grids (grid_position_live close to 1)\n",
    "        sell_multiplier = max(0.1, grid_position_live)\n",
    "        # Example: sell a percentage of BTC holdings, e.g., 10% of holdings * multiplier\n",
    "        max_sell_btc = current_btc_held * 0.10 \n",
    "        amount_to_sell_btc = max_sell_btc * sell_multiplier\n",
    "        \n",
    "        value_of_btc_to_sell = amount_to_sell_btc * current_price\n",
    "        if current_btc_held > 0 and amount_to_sell_btc > 0 and value_of_btc_to_sell >= MIN_TRADE_AMOUNT_USD:\n",
    "             # Ensure quantity meets Binance minimums (e.g. for BTC often 0.00001, check symbol info)\n",
    "            min_btc_qty_binance = 0.00001 # Example, fetch dynamically for robustness\n",
    "            if amount_to_sell_btc < min_btc_qty_binance:\n",
    "                print(f\"SELL quantity {amount_to_sell_btc:.8f} BTC is below Binance minimum {min_btc_qty_binance:.8f} BTC.\")\n",
    "            else:\n",
    "                print(f\"Attempting SELL: {amount_to_sell_btc:.8f} {TICKER} at ~{current_price:.2f}\")\n",
    "                try:\n",
    "                    order = binance_client.create_order( # Use create_order for real testnet trades\n",
    "                        symbol=TICKER,\n",
    "                        side=Client.SIDE_SELL,\n",
    "                        type=ORDER_TYPE,\n",
    "                        quantity=f\"{amount_to_sell_btc:.8f}\") # Binance API needs string for precision\n",
    "                    print(f\"SELL Test Order successful: {order}\")\n",
    "                    # For real orders, update balance:\n",
    "                    # current_usdt_balance += actual_revenue_from_order_fills\n",
    "                    # current_btc_held -= actual_btc_sold_from_order_fills\n",
    "                except BinanceAPIException as e:\n",
    "                    print(f\"Binance API Exception (SELL): {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error executing SELL order: {e}\")\n",
    "        else:\n",
    "            print(f\"SELL condition not met: Sell amount {amount_to_sell_btc:.8f} BTC (value {value_of_btc_to_sell:.2f} USD) too small or no BTC held.\")\n",
    "    \n",
    "    print(f\"Current Portfolio: USDT: {current_usdt_balance:.2f}, BTC: {current_btc_held:.8f}, Value: ~{(current_usdt_balance + current_btc_held * current_price):.2f} USDT\")\n",
    "\n",
    "    # 7. Adapt Grids Periodically\n",
    "    live_steps_since_recal += 1\n",
    "    if live_steps_since_recal >= LIVE_RECALIBRATION_INTERVAL:\n",
    "        if len(live_recent_prices_for_grid) >= LIVE_LOOKBACK_WINDOW_GRID:\n",
    "            print(\"Recalculating grid levels...\")\n",
    "            new_grids = recalculate_grids_live(list(live_recent_prices_for_grid))\n",
    "            if not np.array_equal(new_grids, current_grid_levels) and len(new_grids) > 0:\n",
    "                 current_grid_levels = new_grids\n",
    "                 print(f\"Grid levels adapted. New first 5: {np.around(current_grid_levels[:5], 2)}...\")\n",
    "            else:\n",
    "                print(\"Grid levels unchanged or recalculation failed.\")\n",
    "            live_steps_since_recal = 0\n",
    "        else:\n",
    "            print(f\"Not enough data in live_recent_prices_for_grid ({len(live_recent_prices_for_grid)}/{LIVE_LOOKBACK_WINDOW_GRID}) to adapt grids yet.\")\n",
    "\n",
    "\n",
    "def start_live_trading_bot():\n",
    "    print(\"Initializing live trading bot...\")\n",
    "    if not initialize_live_trader():\n",
    "        print(\"Bot initialization failed. Exiting.\")\n",
    "        return\n",
    "\n",
    "    print(\"Live trading bot initialized. Starting 1-minute trading cycle.\")\n",
    "    print(\"Using TEST orders. For actual Testnet trades, change create_test_order to create_order.\")\n",
    "    print(\"Press Ctrl+C to stop the bot.\")\n",
    "\n",
    "    # Run the first cycle immediately\n",
    "    run_live_trading_cycle() \n",
    "    \n",
    "    # Schedule the job\n",
    "    schedule.every(1).minutes.do(run_live_trading_cycle)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            schedule.run_pending()\n",
    "            time.sleep(1)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nStopping live trading bot...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred in the main loop: {e}\")\n",
    "            time.sleep(60) # Wait a bit before retrying if a major error occurs\n",
    "\n",
    "# To run the bot:\n",
    "start_live_trading_bot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2124ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from binance.client import Client\n",
    "\n",
    "api_key = 'OrR1dfYW9wa8pjjgfOrk5XhdICHZEwFvsKSCgmUgyGrFu7YMfvR2WBhbTMELu858'\n",
    "api_secret = 'JOENNjZJ7PWeVp6GbQBMcBJC95VzjlASitaeSVol1jIzjmIpnLXUWsganegbQZVE'\n",
    "client = Client(api_key, api_secret)\n",
    "client.API_URL = 'https://testnet.binance.vision/api'\n",
    "\n",
    "# Get current market price for BTC/USDT\n",
    "ticker = client.get_symbol_ticker(symbol='BTCUSDT')\n",
    "market_price = float(ticker['price'])\n",
    "\n",
    "print(f\"Current BTC/USDT Market Price: {market_price}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a0191",
   "metadata": {},
   "outputs": [],
   "source": [
    "usdt_to_spend = 10  # for example\n",
    "quantity_to_buy = round(usdt_to_spend / market_price, 6)  # 6 decimal precision\n",
    "print(f\"Quantity of BTC to buy with ${usdt_to_spend}: {quantity_to_buy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8c9b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.order_market_buy(symbol='BTCUSDT', quantity=0.001)\n",
    "client.order_market_sell(symbol='BTCUSDT', quantity=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.order_market_buy(symbol='BTCUSDT', quoteOrderQty=100)\n",
    "client.order_market_sell(symbol='BTCUSDT', quoteOrderQty=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
